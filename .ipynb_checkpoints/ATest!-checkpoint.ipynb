{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from scipy import io as sio\n",
    "from tensorflow.python.framework import ops\n",
    "from dfs2 import DeepFeatureSelectionNew\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# ourdataB = sio.loadmat(\"/Volumes/TONY/Regeneron/Data/OriginalData/newDataB_2labels.mat\")\n",
    "ourdataB = sio.loadmat(\"/Users/xupeng.tong/Documents/Data/OriginalData/newDataB_2labels.mat\")\n",
    "\n",
    "inputX = ourdataB['X']\n",
    "inputX = normalize(inputX, axis=0)\n",
    "inputY = ourdataB['Y'][0,:]\n",
    "columnNames = ourdataB['columnNames']\n",
    "\n",
    "# iris = datasets.load_iris()\n",
    "# inputX = iris.data[:,[1,2,3]]\n",
    "# inputY = iris.target\n",
    "\n",
    "# digits = datasets.load_digits()\n",
    "# inputX = digits.data  \n",
    "# inputY = digits.target\n",
    "\n",
    "# inputX, inputY = make_classification(n_samples=1000, n_features=7500, n_informative=3000, n_redundant=0, n_repeated=0, n_classes=2)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputX, inputY, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indexes = sio.loadmat(\"xgboost_result\")['importance_rank']\n",
    "\n",
    "\n",
    "X_train500, X_test500 = X_train[:, indexes.tolist()[0][:500]], X_test[:, indexes.tolist()[0][:500]]\n",
    "X_train100, X_test100 = X_train[:, indexes.tolist()[0][:100]], X_test[:, indexes.tolist()[0][:100]]\n",
    "X_train10, X_test10 = X_train[:, indexes.tolist()[0][:10]], X_test[:, indexes.tolist()[0][:10]]\n",
    "X_train50, X_test50 = X_train[:, indexes.tolist()[0][:50]], X_test[:, indexes.tolist()[0][:50]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(indexes.tolist()[0][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: global loss = 0.683401703835\n",
      "('Train accuracy:', 0.58020359)\n",
      "('Test accuracy:', 0.59382313)\n",
      "epoch 100: global loss = 0.0135009465739\n",
      "('Train accuracy:', 0.99649)\n",
      "('Test accuracy:', 0.9948526)\n",
      "epoch 200: global loss = 0.00880366563797\n",
      "('Train accuracy:', 0.998362)\n",
      "('Test accuracy:', 0.9948526)\n",
      "epoch 300: global loss = 0.00728926341981\n",
      "('Train accuracy:', 0.99871302)\n",
      "('Test accuracy:', 0.99532056)\n",
      "epoch 400: global loss = 0.0063290707767\n",
      "('Train accuracy:', 0.99883002)\n",
      "('Test accuracy:', 0.9948526)\n"
     ]
    }
   ],
   "source": [
    "ops.reset_default_graph()\n",
    "\n",
    "weights = []\n",
    "for lambda1 in xrange(0, 100, 5):\n",
    "    lambda1 /= 1000.\n",
    "    dfsMLP = DeepFeatureSelectionNew(X_train100, X_test100, y_train, y_test, n_input=1, hidden_dims=[100], learning_rate=0.01, \\\n",
    "                                     lambda1=lambda1, lambda2=1, alpha1=0.0000001, alpha2=0, activation='tanh', \\\n",
    "                                     weight_init='uniform',epochs=500, optimizer='Adam', print_step=100)\n",
    "    dfsMLP.train(batch_size=2000)\n",
    "    print(\"Train finised for lambda1:\" + str(lambda1))\n",
    "    weights.append(dfsMLP.selected_ws[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.argsort(dfsMLP.selected_ws[0])\n",
    "\n",
    "selected_w = dfsMLP.selected_ws[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfsMLP.refine_init_weight(0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZwAAAEACAYAAACH5cABAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJztvXmYZVV19/9Z1fM8AN1Nd9NNTwzNGFRsh2AZjQwmtBk0\nEBzQvEqCiI8ag0QCjb83UfMm8Q0hBDCYVwwGURFQEJFgqYlhspnphm6wkR4ZeqquHqq6av3+2Hdz\nT50647333HF9nqee7nvqnHP3PXXu/p7v2muvLaqKYRiGYRRNV6MbYBiGYXQGJjiGYRhGXTDBMQzD\nMOqCCY5hGIZRF0xwDMMwjLpggmMYhmHUhcIFR0TOEJG1IvKsiFwSs89VIrJORB4VkZPTjhWRL4jI\nYyLyiIjcLSJzStsXisheEVld+rmm6M9nGIZhZEOKnIcjIl3As8A7gM3AQ8A5qro2sM+ZwEWq+m4R\neSPwj6q6IulYEZmsqntKx38CWK6qfyYiC4Hvq+qJhX0owzAMoyKKdjinAutU9QVVHQBuBlaG9lkJ\n3Aigqg8A00RkdtKxXmxKTAKGAq+lkE9iGIZhVEXRgjMPeDHwemNpW5Z9Eo8Vkf8tIr8G/hi4PLDf\nkaVw2k9E5K3VfwTDMAyjFjRj0kAmh6Kql6nqAuAm4BOlzVuABap6CvAZ4JsiMrmYZhqGYRh5GF3w\n+TcBCwKv55e2hfc5ImKfsRmOBfgmcBewSlX7gX4AVV0tIs8BRwGrgweIiBWQMwzDqABVrXjYomiH\n8xCwtJQ9NhY4B7gjtM8dwAcBRGQFsFNVtyUdKyJLA8e/B1hT2n5oKdkAEVkMLAWej2qYqtqPKldc\ncUXD29AsP3Yt7FrYtUj+qZZCHY6qDorIRcA9OHG7QVXXiMgF7td6vareJSJnich6oA/4cNKxpVN/\nSUSOwiULvAD8aWn7acAXRKS/9LsLVHVnkZ/RMAzDyEbRITVU9W7g6NC260KvL8p6bGn7H8bsfytw\na8WNNQzDMAqjGZMGjDrS3d3d6CY0DXYtyti1KGPXonYUOvGzWRER7cTPbRiGUQ0igjZx0oBhGIZh\nACY4hmEYRp0wwTEMwzDqggmOYRiGURdMcAzDMIy6YIJjGIZh1AUTHMMwDKMumOAYhmEYdcEExzAM\nw6gLJjiGYRhGXTDBMQzDMOqCCY5hGIZRF0xwDMMwjLpggmMYhmHUBRMcwzAMoy6Y4BiGYRh1wQTH\nMAzDqAsmOIZhGEZdMMExDKMmfPvbMDDQ6FYYzYwJjmEYNeHjH4eenka3wmhmTHAMw6gJAwNw552N\nboXRzBQuOCJyhoisFZFnReSSmH2uEpF1IvKoiJycdqyIfEFEHhORR0TkbhGZE/jdpaVzrRGRdxX7\n6QzD8Bw8CN//Pqg2uiVGs1Ko4IhIF3A1cDpwHHCuiBwT2udMYImqLgMuAK7NcOzfqupJqvobwJ3A\nFaVjlgPvA44FzgSuEREp8jMahuEYGIDeXnjmmUa3xGhWinY4pwLrVPUFVR0AbgZWhvZZCdwIoKoP\nANNEZHbSsaq6J3D8JGCo9P+zgZtV9aCqbgDWlc5jGEbBDAzA2WdbWM2Ip2jBmQe8GHi9sbQtyz6J\nx4rI/xaRXwN/DFwec65NEe9nGEaNGRpyPytXwg9+0OjWGM3K6EY3IIJMITBVvQy4rDS28wlgVZ43\nWbWqvHt3dzfd3d15DjcMI8DBgzB6NLzjHXDeebBzJ0yf3uhWGdXS09NDTw1TD4sWnE3AgsDr+aVt\n4X2OiNhnbIZjAb6JG8dZlXCuEQQFxzCM6jh4EMaMgYkT4Td/E+65B973vka3yqiW8MP4lVdeWdX5\nig6pPQQsFZGFIjIWOAe4I7TPHcAHAURkBbBTVbclHSsiSwPHvwdYGzjXOSIyVkQWAUuBB4v5aIZh\neAYGnMMBePe7LaxmRFOow1HVQRG5CLgHJ243qOoaEbnA/VqvV9W7ROQsEVkP9AEfTjq2dOovichR\nuGSBF4A/LR3ztIjcAjwNDAAXqlqSpmEUzcCAczjgBOeKK2BwEEaNamy7jOZCOrE/FhHTIcOoIVu3\nwkknwbZt7vUJJ8D118Ob3tTYdhm1RURQ1YqnmlilAcMogP5+uPvuRreifgQdDsDb3w7/8z+Na4/R\nnJjgGEYBrF0LF13U6FbUD5804Jk+Hfbsid/f6ExMcAyjAPys+04hmDQALltt797GtcdoTkxwDKMA\nBgZg9+5Gt6J+hENqJjhGFCY4hlEAAwOwf3/nrA/jJ356THDK7NnjxvQMExzDKISDB92/nRJWi3I4\nfX2Na08zceml8I1vNLoVzYEJjmEUgHc2nRJWCycNTJpkDsfT22sJFB4THMMogE4THEsaiKe/30Jq\nHhMcwygALzidElILOxwTnDImOGVMcAyjAPwYTic7HBvDcZjglDHBMYwC6MSQmo3hRGOCU8YExzAK\nwEJqJjgeE5wyJjiGUQCd6HAsaSAaE5wyJjiGUQCdJjhRDsfGcBz9/XDgQKNb0RyY4BhGAXTixM+g\nwxk3zl0Dfx06GXM4ZUxwDKMABgbcU36nOJxw0oCI+/z79jWuTc2CCU4ZExzDKICBATjkkM4RnHBI\nDWwcx2OCU8YExzAKYGAAZs7s3JAamOB4THDKmOAYRgEcPOgEp9MdjiUOmOAEMcExjALotJCaOZx4\nTHDKmOAYRgH4kFonCU7Y4Vi1AYcJThkTHMMoAO9wOmUMx5IG4jHBKWOCYxgFMDAAM2Y4h6Pa6NYU\nT1xIrdPHcFTdtTHBcRQuOCJyhoisFZFnReSSmH2uEpF1IvKoiJycdqyI/K2IrCnt/10RmVravlBE\n9orI6tLPNUV/PsOI4uBBF1IaNcotNd3umMOJxlecsEoDjkIFR0S6gKuB04HjgHNF5JjQPmcCS1R1\nGXABcG2GY+8BjlPVk4F1wKWBU65X1VNKPxcW9+kMIx4/pjF1ameE1aIcjo3hlJ2NORxH0Q7nVGCd\nqr6gqgPAzcDK0D4rgRsBVPUBYJqIzE46VlXvVdWh0vH3A/MD55PCPo1hZMQLzpQpnZE4EJU0YA7H\nBCdM0YIzD3gx8HpjaVuWfbIcC/AR4IeB10eWwmk/EZG3Vtpww6iGoMPpBMGxkFo0JjjDGZ2+S93J\n7FBE5PPAgKp+s7RpM7BAVXeIyCnAbSKyXFX3hI9dtWrVa//v7u6mu7u7qkYbRpCDB12IqZNDahMn\nwq5djWlPs9DfD5Mnt67g9PT00NPTU7PzFS04m4AFgdfzS9vC+xwRsc/YpGNF5HzgLOC3/LZS6G1H\n6f+rReQ54ChgdbhhQcExjFrTaSG1OIezZUtj2tMseMFp1Xsg/DB+5ZVXVnW+okNqDwFLS9ljY4Fz\ngDtC+9wBfBBARFYAO1V1W9KxInIG8FngbFV9Lf9DRA4tJRsgIouBpcDzRX5Aw4ii00JqljQQTas7\nnFpTqMNR1UERuQiXVdYF3KCqa0TkAvdrvV5V7xKRs0RkPdAHfDjp2NKp/wnngH4sIgD3lzLSTgO+\nICL9wBBwgaruLPIzGkYUnZilZmM4I+nvd8J78CAMDUFXh898LHwMR1XvBo4Obbsu9PqirMeWti+L\n2f9W4NaKG2sYNcI/8XeKw/FjVkFs4qcTnHHjnBgPDLj/dzIdrreGUQx+TKNTxnDM4UTT3w9jx7of\nC6uZ4BhGIXTaGE5U0oCN4ZQFZ9w4qzYAJjiGUQidOIZjyxOMxBzOcExwDKMAOi0t2kJq0ZjgDMcE\nxzCq4A/+AF55ZeT24MTPThAcSxqI5sABE5wgJjiGUQUPPBAtOJ0YUjOHMxJzOMMxwTGMKohbXKvT\nQmqWNBCNCc5wmrGWmmG0DAcOJAvOmDGdIThRSQPjxrlrMzjo1gXqRExwhpPqcETkvSIypfT/y0Tk\n1lJhTMPoeJIcTqcV7ww7HBELq5ngDCdLSO2vVLW3VOr/ncANwL8U2yzDaH5U4x2ODzFNngx79riy\nJu1MVNIAmOCY4Awni+AMlv59N3C9qt6Jq2NmGB3N4KATnaSQ2qhRMGFC+2drRTkcSB/HeeABePjh\n4trVaIITP01wsgnOJhG5Dvgj4C4RGZfxOMNoa/zM8STBgc4Iq0UlDUC6w7n1Vrj99uLa1WiCDscq\nDWQTjvcBPwJOL1VenolbGsAwOpqk1RyDgtMJmWpRSQOQPhdn717Yt6+4djUaC6kNJzFLTURGAatV\n9Ri/TVW3AB2+rJJhxDscVRdu8x1wJ0z+jAuppTmcvXvdtWpX+vth+nQTHE+iw1HVQeAZEVmQtJ9h\ndCJxDufgQTd2I6XF0jtBcCpNGti3r72TCszhDCfLPJwZwFMi8iBugTQAVPXswlplGC1AnMMJP+1P\nmdL+YziVJg3s3evEql0xwRlOFsH5q8JbYRgtSJzDCXe+neJwKg2pmeB0DqmCo6o/FZGFwDJVvVdE\nJgIdOm/YMMokOZxgeKkTBKfSpIF9+0xwOokslQY+CnwH8MtCzwNuK7JRhtEKJI3hhB1OO4fU/KB/\nVPmaLA7HxnA6hyxp0R8H3gLsBlDVdcCsIhtlGK2AdzgDA8O3R43htLPDiUsYgPQxHEsa6CyyCM4B\nVX3tUonIaECLa5JhtAY2huOISxgAczhWaWA4WQTnpyLyl8AEEflt4NvA94ttlmE0P1mz1No9pBaX\nMADZJn52guBYpQFHFsH5HPAy8ARwAXAXcFmRjTKMViBpDCcYYmr3kFpcwgDYPBwLqQ0nVXBUdQj4\nOvD/AVcCX1fVzCE1ETlDRNaKyLMicknMPleJyDoReVRETk47VkT+VkTWlPb/rohMDfzu0tK51ojI\nu7K20zDyksfhtLPgpDmcOEFR7SyHY4KTLUvt3cBzwFXA1cB6ETkzy8lFpKt0zOnAccC5InJMaJ8z\ngSWqugznoK7NcOw9wHGqejKwDri0dMxyXO23Y4EzgWtE/Hxvw6gtecZw2jmkluRwkpIGBgagq8tl\nubVrarQJznCyhNT+Hni7qnar6tuAtwNfyXj+U4F1qvqCqg4ANwMrQ/usBG4EUNUHgGkiMjvpWFW9\nt+S8AO4H5pf+fzZws6oeVNUNODE6NWNbDSMXBw64VOAslQba2eFUmjSwd6/7/cSJ7VvA0wRnOFkE\np1dV1wdePw9kfV6bB7wYeL2xtC3LPlmOBfgIblwp6lybYo4xjKrp73dikmUeTjsLTlJadFLSQFBw\n2jWsVm/B+fSnYUsTl1aOrTQgIr9f+u/DInIXcAsuHfq9wEMFtilzCExEPg8MqOp/5H2TVatWvfb/\n7u5uuru7857C6HAOHIgWnHarNHDVVfDxj0dP7ITKHc6+fe73Q0MmOLXizjvh938fDj+8Nufr6emh\np6enNicjubTN7wb+vw14W+n/LwMTMp5/ExCsND2/tC28zxER+4xNOlZEzgfOAn4rw7lGEBQcw6iE\n/n63hHRaSG3iRCdOSU6gmfnzP4eVK2HhwujfV5o0sHevWw3VJw+0I/39bg5OvQRnzx7Ytat25ws/\njF955ZVVnS/29lfVD1d1ZsdDwNJSLbYtwDnAuaF97sBVM/iWiKwAdqrqNhF5Je5YETkDtwjcaap6\nIHSum0TkK7hQ2lLgwRp8DsMYQZLDCXbAIuWK0TNm1LeN1dLf7z7Ppk3xglNp0oAPqbW74NTT4fT1\n1VZwak3q85aILAI+ARwZ3D/L8gSqOigiF+GyyrqAG1R1jYhc4H6t16vqXSJyloisxy1/8OGkY0un\n/iecA/pxKQntflW9UFWfFpFbgKeBAeDCPCnchpGHrA4HymG1VhOcPXvcv5si4wSOtJBa3BiOD6l1\nguDUo9KAqvt7NXP4NovBvw24AVddYChl3xGo6t3A0aFt14VeX5T12NL2ZQnv90Xgi3nbaRh58Q7n\n1VeHb48KnbXqmjhecDZvjt8nLWkgLaTm/9+O1LPSQH+/SzFvaYcD7FfVqwpviWG0GN7hhLOCop74\nx4+H/fvr17ZaUa3DGT++3BGGkw46xeGMGVOfkJr/W7W64PyjiFyBC229ptGqurqwVhlGC3DgQPaQ\n2rhx7Ss4SUkDIuV5NpMnD/9duzucwUH3+UeNqo/g+NBlq4fUTgA+gMsG8yE1ZXh2mGF0HP39cOih\n2QRn/PjWLN64Z48Ll6U5nKTsOx9WixKciRPL/283fDgNzOF4sgjOe4HFwSUKDMOIz1KLeuIfN651\nBWfx4spDahCfOOBDamCCUwv8NW5mwclSaeBJYHrRDTGMViMpSy38xN/KgnP00U5w4vI90+YXxSUO\n+JBau1YaaITD6epqbsHJ4nCmA2tF5CGGj+GkpkUbRjuTdR4OtLbgzJnjPs/OndFp3VkcTpSgeIcj\n0p611BrhcObMaf0xnCsKb4VhtCB55uG0suBMngzz5jmXEyU4SUkDED/5c+9emD7dCc727bVrc7PQ\nCIczd+7INP1mIst6OD+N+qlH4wwjjqeeghtvbGwbOsXhBAUniixJA1FjOJ0UUss78XP9elcTLQ99\nfU5wmjmklmU9nF4R2V362S8igyLSxKbN6ATuvx++973GtiHO4USNabRyWnQWwakmpNYJgjN6tLsv\nhjJOnd+4ER55JN/7eYeze3f8eFujSQ2pqeoU///SYmYrgRVFNsow0ti1q/Fx/+A8HFUXGgLXAU+a\nNHzfVk6LPvLIZMGpNmlg1Kj2FxwR9/+BAffwkUZfH7zySr736+tzIc/Ro4dnADYTWbLUXkMdt+FW\n4TSMhrF7d3MIju8wgytWdmJIrdIxnGZ3OK++CqsrnOIeFBzIV96mr89d+zz3jP9bTZvWvGG1LMU7\ng5HELuD1QAsGB4x2ohkcTrj0vO9021Fwpk2DH/4wep+0pIG0eTjN7HDuvhu+853KwrcHDowUnKzj\nOP56vPqqC5Nloa8PZs8uF4qt1Zo4tSRLllpwXZyDwAZGLhNtGHWlGQTHdyg+VOKJm/jpZ4K3El5w\nZs6sLmmgVUNq+/bFV7tOI8rhZBUc/56vvJJdcNrC4dRoXRzDqCm7dze+k4pbXKvdJn5OmlR90kBU\npexWcDh79zZecLLS1+f+Vi0tOCJyGPBRRq6H85HimmUYyTSbwwkLTjsV75w8GWbNgh07oj9blpDa\ntm0jt/sxnGYWnEY7nDxzavzDQUsLDnA78HPgXmCw2OYYRjaaIWkgyeG0U/HOyZOdKMye7ZZiWLBg\n+D4DA8kZUUlJAxMmODdogjOcSh3O5MnlMZxmJIvgTFTVSwpviWHkoBUdTisLDrixhE2bRgpOtUkD\nJjgj6etzVRjyCE4rOJwsadE/EJGzCm+JYeRg927XgWedSFdrBgfde48ePbIjiZv42eqCEzeOU23S\nQKPTogcH4W/+Jvp3tRScPNUG+vpg4cJ8ITXvcFpdcD6JE519pWoDvVZpwGg0u3Y1tuij70z8hL52\ndDiDg27cyS+SliQ4eSsNDA256zF+vDtWZHimXz3Zvh0uuyx6dr5PGqhk5n61DmfBgsocTjOH1LLU\nUpuiql2qOkFVp5ZeT61H4wwjioEB11lNn95YwfEzxttVcHzWU1epl4gTnLRKA1FjOPv3O7Hx526k\ny9mxwwlK1N9n3z4nvJUU3qxWcBYurGwMp9UdjmE0Fb29rmimX7q4EQQn9bWr4ATDaVC5w5k8eeQT\nd3B5aWis4PhK1XH13qCysFq1lQbyhtTaZQzHMJqKXbvcl2rChOZ0OFGD6OPHt15adFbBSXM4M2c6\nFxEkXOurUwSnqJDawIBzYuPGtXhIzaicJ5+EO+9sdCvaj9273ZeqkYKT5nDaIWmgVg5n5syR6934\nOTieZhUcv60RgpMnpObDnyJt4HBE5K0i8uHS/w8TkUVZ30BEzhCRtSLyrIhEpleLyFUisk5EHhWR\nk9OOFZE/FJEnS0slnBLYvlBE9orI6tLPNVnbWQQ//Sl861uNbEF7EnQ4jeqkOmEMJ05wwgPoWYp3\nDgwMd3jNFFLz7quZHM7evW71zgMHst03fvwGWlxwROQK4BLg0tKmMcC/Zzm5iHQBV+OqSx8HnCsi\nx4T2ORNYoqrLgAuAazMc+wTwe0DUQnDrVfWU0s+FWdpZFPv3V55SacTjHY6N4RRLWHAmTy4vNR0k\nLaQmMjKsFg6pNfLhIS2kNnVqYxzO5MlwyCHZxnH8+A249ras4OA69rOBPgBV3QxMSTyizKnAOlV9\nQVUHgJsZWfhzJXBj6dwPANNEZHbSsar6jKquAyTiPaO2NYR9+5p3Qlsr0+xjOO0qOBAdVktzODAy\nrNZMDidNcA49tDGCM2mSE5wsYbWww2nlMZx+VVVAAURkUsr+QeYBLwZebyxty7JPlmOjOLIUTvuJ\niLw1R1trjglOMTSD4AQdzpgx7TnxM6vgpDkcGCk4zZQ0kBZSq5XgZJ34efCg+xk3zr13XoczaZK7\n1xo1rymJLKVtbhGR64DpIvJR4CPAVwtsUzUOZTOwQFV3lMZ2bhOR5ao6ojD8qlWrXvt/d3c33d3d\nVbxtNNXMUjbi8SG13t7WcjitnqUGrrzN5s3Dt1XqcIoQnG3b3H2xdGn2Y7Zvjw/p7d0Lhx1WX4fT\n1+euh4gTnLwOR6ScqXbIIfnbHaSnp4eenp7qThIgy/IEfycivw3sBo4GLlfVH2c8/yYgWHlpfmlb\neJ8jIvYZm+HYcFsHgB2l/68WkeeAo4ARa/YFBacozOEUg3c427c37vrmHcNpxeKdUYIzbdrIpQaa\nKaR2003w85/nWzBt+3aYP795Qmo+nAaVjeFA7QQn/DB+5ZVXVnW+LA4HVf2xiDzg9xeRmaq6PeUw\ngIeApSKyENgCnAOcG9rnDuDjwLdEZAWwU1W3icgrGY6FgCMSkUOB7ao6JCKLgaXA81k+YxGY4BTD\n7t0utNPIpIG8DseHnLKEn5qFKMGJEoZmCqnt3g3/9V8uk04yxkq2b3f3U1T5nf5+1/ZGCU4lDgea\nN1MtS5baBSKyFXgceBj4ZenfVFR1ELgIuAd4CrhZVdeUzvmx0j53Ab8SkfXAdcCFSceW2vQeEXkR\nWIGr8+YXvz0NeFxEVgO3ABeoaiinpn5YSK0Ymm0MJ8vET2i9cZwowZk0aeQ9ncXhzJhRn5Bab6/r\noNeuzX7Mjh3RDmf/fvc3mzy5vpUGKhGcsMNpVsHJ8qz158Dxqpqjqk8ZVb0bF4oLbrsu9PqirMeW\ntt8G3Bax/Vbg1kraWQT795vDKYJmmPiZ5nCinvi94EzKk3bTQMJPzeCEIW9aNDiX8OST5df79o0M\nqdViCe7eXteWn/8cjj02fX/VeIfjRXHSpHw1zTy1CqmtHjEgEH1M8G/VrNUGsmSpPQdYt1kBPqRW\nSaVZI55mdzhxT/zt4HCinEgzJQ3s3g2nneYEJwt79ri/y4wZI+8lL4pRri4L9QyptZPDuRT4RWkM\n57Wvi6peXFir2gR/A4fj1Z3Inj3O8R16aPXn2r279SoNQOtlqtVacIITP/fudSuIJp23Enp74ayz\n4Kqrsu2/Y4cTm4kTYePG4b9rBsHJMw9n/vzy62YVnCwO5zrgPuB+3PiN/zFS8IJjYTX46lfh9NPT\nF0z7wQ8gLYFw167WqzQArZeplnUMp5mSBnp74fWvd+f/9a/T99++3bUt6v2bQXAqmYcDzRtSy+Jw\nxqjqpwtvSRtiglPm1VddLPqmm+ADH4jfb9264bH+KJohpBbncIaGXAi1K+JRzkJq5ddFJg1MnQq/\n+ZsurHbeecn7pwmOH8Opp+Ds3dvBWWrAD0XkYyJyuIjM9D+Ft6wN2LfPPflZppobaD73XPj855M7\nlt7ekYPSQVSzJQ0MDbniqX/yJ+mOqRLiHI7vfKNScttVcCpxOOF5OLUKj+7e7dZK8oKTRjCkFpU0\nUEuHk7XSQNDhTJ3q7vG041plDCeL4JxLaRyHcjgtU1p0p7N/v4vBmsNxX+yzzoIVK+ArX4nfr7c3\n+YviRXzs2PhO6rvfhcWL4ROfcMJTw4nSrxF2OL6MSNLTfrsKThaHM22aO9/Bg+51kSG1oMNJo9lD\naiLZJn+2jcNR1UURP4vr0bhWZ98+ExzPzp1uSegvfckJztat0fulORzvbiDe4dx0E1x6KTz+OFx8\ncfL5KiXN4UTRroITN+8oSFeX6wT936KoSgN+NdiTTnJJAGnhqGYVnKAYh8NqV10F//qvw49plTGc\nLBM/x4jIxSLyndLPRSKScnsZUF1ZjHbDC87ixXD++XD55dH7pQmOH7+B+KSB3bvd+4B7zyKe9OLG\ncJI631YSHNWRnRjET/zMUj0hGFYrwuEMDLif8eNde970Jld1IInt2+NDas3gcGBkptrNN8PDD488\npi0cDvAvwOuAa0o/ryttMxIIlsUwh+NEZMYM9//PfAa+/e3o/bzgxM1d8inREO9wgi4o+FRdS5Ic\nTlzn20rLTO/f7z5X+LNUGlKD4YJTRNKAD6f58bMsYbUdO+IdTnDiZ6MqDcDwTLVdu+DBB0emcLfT\nGM4bVPVDqnpf6efDwBuKblir48tiTJqU/kV65hm45Zb6tKtR7Njh3Aa46ru7d0eLSm+vcwlxyQA+\nJRriBcd3PFAOLaSlY+clzuG0S0gtKpwGIztm1ez14cKCU+uQmg+nebIITrOG1MKC4x1OT497HRac\ndqo0MCgiS/yLUlHMweKa1B74m3XixPSb9Re/gC9/uT7tahQ+pAaucxo/Pvq6+ErEca4kGFKLSxrw\nmUr+vSZNqk3ZlCDtPoaTJjj+YWFw0I3PRKWBhwnWUysipBb8uwO84Q3wxBPJnXyWkNr48eU1avJQ\nREjt3nvh/e9vb4fzWeAnItIjIj/FTQL9TLHNan3273c3ahaH09cHjz3WvqE3vxhUsIOJW5WwtxdG\njYoXnCxJA0GH49+r1mG1TnU4o0a5z+c/R5aEAU9SSM0/PFRTBir8d58wARYscHO74vAhtaiHFy84\nItkeHIOojrwXqnE4PqR2771uekFwLajBQff3CDpG73CaraxWliy1/wSWARcDnwCOVtWfFN2wVifo\ncNKEZO9ed9P8sk3rN+za5Z4ig3NT4tZd7+11i3xlcThRSQNDQyPDC9On115w4hxOUnipHQQHht/T\nWRMGYGTQ+KPlAAAgAElEQVTSQLCDHD165MqpeQmH1ACOPx6eeir+GB9S81UggqHXoCjmDatFzceq\n1uH4rLvf+I3hK6/68GTQZSZFERpJliy19wJjVfVx4GzgP0qraRoJ5Amp+d8/8EDx7WoEwfEbT1yM\nefduOOKI+HBA0OGMHes6+MFAgHfPHnfNg1++IsILnepwYKTg5HU4AwOuYw8fV21YLRxSAzjuuOTK\nFT6k1tXlOujgA0xQFPMKTjicBtU7nHvvhd/6LdfW+fPLYbWoqt7QnGG1LCG1v1LVXhF5K/AO4AYs\nSy2V4IBjFodz8slw//31aVu9CY7feKJCakND7lrMm5fN4YiMDKuFwypQX4eTJjitkqWWVXDyLCjn\nC3gGQ1Vx562EqL/98cfHC87AgGuLPyb8/rUWnEoqDUA5aeDee+Gd73TbgoITlb4OrSs4/vnx3cBX\nVfVO3PLPRgJ5Qmp9fe7JpZMEJyqk5t3JzJnJYzhecGBk7D3qKbcIwanE4bRS8c4iHU5c9fRqy9vE\nhdTiBMeXtfHCV7TgVFJLDVxI7eWXRwqOD6m1m8PZJCLXAX8E3CUi4zIe19HkDakdf7y7GcPZJ+1A\ncA6OJyqk5juMJIEIpkVDNodTxBcvaQynFUJqqq7OXFy6eJLgBDvfSpIGwgkDniJCasuWwYsvRieX\n+PGbuPdvppDahg3u77FokduWxeE0Y2p0FuF4H/Aj4PTScs0zcZlrRgJ5stT8E80b39ie4zhZx3CC\nghMnEMGQGoxMHGgGh9MKSQP798PXvlZOQw9TZNJAeA5O1HkrIcrhjBnjRGfNmpH7+/GbuPevJmmg\nUsFRHSnI06a57EDvbqCNx3BUda+q3qqq60qvt6jqPcU3rbXJG1KbNMkVtmzHsFrcGE74y+A7jKQ0\n5mDSAIx0OOHfQzHlbVp9Ho6flxR3nRsRUitiDAfiM9V8SnTc+1fjcIL3h2f0aOcIkyYh79vnjhs1\nqrzNF/CME5x2G8MxKiBvSG3ixM4SnGpCauExnCwhtWYYw2kmwUmbYFtE0sCMGa6TDxenjDpvJUS5\nW4jPVIsKqRWZpSaS7nLC4TTPZZe5BQw9WRxO1pDawED95gCa4BRE3iy1SZPczOhHHimXum8X8ozh\nTJ2aLDhRDqcRSQPBJ1g/fyRqsl+QZspSq8bhBDvfPA5nzBjXqW/bVr+QGsQnDqSF1KoVHP9AEqRS\nwfnEJ4Z/ttmzXap0f3/1DucLX4C/+Iv0/WqBCU5B5A2pTZzoOtIjj3TlONqJqDGcqLToWozhxKVF\nFzkPp6vLhUB8+ZNWyFKrVUgtj8MB18Fv3Fj/kFpWh1PkGA5ULjhhRo2COXNgy5Z4h+N/n8TAgFvq\n4Fe/Sn/PWmCCUxB5Q2r+JmvHsFrWtOi0MZzBQdcJBL9cUWM44afcIkJq4Ri970haJWmgViG1PA4H\nXAe/cWMxDicupLZokZvHEn7AKXIMp2jBATdfbePGeIezaBE8/3zyOX7wA/e98inWRVO44IjIGSKy\nVkSeFZFLYva5SkTWicijInJy2rEi8oci8qSIDIarHojIpaVzrRGRdxX3yZKpJEsN2jNTrVZjOL29\nrhMMVhHImjRQ5BgODBecVhjDaUTSALgOftOm4hxOlOB0dcHy5SMTB4oOqUUJTtrkzzyC48dx4hzO\n4sXpgnP99S6c1haCIyJdwNXA6cBxwLkickxonzOBJaq6DLgAuDbDsU8Avwf8NHSuY3Fp3McCZwLX\niEStLl88lYTUoBiHk7bqYdFkrTSQFlILT/qExszDUXWdRlRhxlYRHO9wduyI/n1RITXvcOoZUoPo\nTLV6z8OB2jocLzhxDmfBAti8OX5M+IUX4KGH4M/+zH236nFvFu1wTgXWqeoLqjoA3AysDO2zErgR\nQFUfAKaJyOykY1X1mVKadlhMVgI3q+pBVd0ArCudp+4ES5sfODC83leQoSHnhvwXcPly91RSy8SB\nE05o7ITSuKSBsAj4kMiECa4jC38BwpM+oTFJA15Ugk7LdyStMvGzEUkDUHY4cSG1uHWQshAXUoPo\nTLWkkJofj/Oi0cyCE+dwxo6Fww93E1+juOEGOO88935z5jhxKpqiBWceEPy4G0vbsuyT5di099uU\n4ZhI+vvhW9+q5EhHuLR53Bdp3z4nSr7zGjUqvpJyJQwNuayg556rzfnyouq+2GFnkhRSE4l2JeGE\nAciWNDB+vDtnrTLEouZY1HMMR9WtoVRN59zbC7Nm1T9pYOZM17HV2uEMDrq/b1xnHZU4kBRS83OF\nfHykXoITLmuTRJrDgfiw2sGDTnA++lH3Olh9ukhy3Cp1oy4hsFWrVr32/+7ubrq7u4f9fv16+NSn\n4I/+qLLzB+24v5GjvsBRcxL8E/mhh1b23kH8mhgbNsDb3lb9+fKyb58T0fHjh2+fMsV9UYaGymIb\njMH7azBrVvmYqPGZLEkDUE4cmDOn+s8UlfKaJaRWiyWmV6+Gz34W7rsPvvc9eM97KjvPnj2uKncj\nxnDCayN5qqml5tsbF0CPE5w4hxNePqGZHc7UqfF/qzjBuesuWLjQXReIF5yenh56enqyNSgDRQvO\nJmBB4PX80rbwPkdE7DM2w7FR7xd1rhEEBSeKHTuqC8OEBSfuZo16oqllCMjH6DdsqM358hI1fgNO\nhCZOdB2FF5Gw4GRxOBMmDL9WUaLkz1crwUlzOEWE1AYHXe2zH/0IrrgCliyJD5VkobfXdViNEByo\nfZZaXMKAZ948J/avvOIe5FRHhnrTBCdP25IEJ+keqERwFizI73C+9rWyu4F4wQk/jF955ZXZGhdD\n0SG1h4ClIrJQRMYC5wB3hPa5A/gggIisAHaq6raMx8JwR3QHcI6IjBWRRcBS4MFKGr5zp7vpKl0Q\nymepQfLNGnWDFSE49cqzDxM1fuMJh9WCnUZUKnMWhxM3cFzLuTiVOpxqBOe55+A//xOefRb+9E9d\nyms143JJDqe/3znPqA4TRhbvzBtSg9qH1JLGb8A5nxNOgJ+W0ox6e933M/gZW83hHH64C5fv3p3P\n4ajCf//38MoF9QqpFSo4qjoIXATcAzyFG9BfIyIXiMjHSvvcBfxKRNYD1wEXJh0LICLvEZEXgRXA\nD0Tkh6VjngZuAZ4G7gIuVK1skVX/Ray0k4oKqUWRFFILc+CAyyzJw44drgNslMOJmvTpCWeqRYXU\ngsQ5nLSkAf9etRLxOIczMFBc0sCGDXD00eXPdsQR1TmcPXviHY4fhI4LTzWrw4nLUPNceSVcfLHr\npMPjN+H3DxfQrKfgRIlxFGPHuhprzz+fz+Fs2uTC2IcfXt7m5/QUTeFjOKp6N3B0aNt1odcXZT22\ntP024LaYY74IfLHS9nqCgnPYYfmPLyKkdtdd8NWvun+zsmOHy9BppMOJE5xwckSa4ESlRQeTBg4e\njB84rqVrTHM4cR1GNYLzwgsu5u6pVnB6e+MdTlI4DapPGvDnSDpvXtJCauDWnDr/fPjQh+Cv/3r4\n+E34/VvB4YB7aHj44fi/16JFI7/7jzzilqkOPlAE19cpEqs0EEMtHU5aSC2rw3nlFVc/KQ87dsCJ\nJ7oSF42o0ZYmOGGH459S48ZwkkJqSQPHtQypVTuGU4nn3rDBlT3y1MLhzJ1bTtwI/y6r4FTqcOod\nUvOsWuX2vfzy2gqO6vCpD/UUHN++KA491L1fsD/xghOkLUJqrYz/A1X6VJw1pBbncKI6x+3b3U8e\nduxwmV5J+fhFUssxnLiQmhecuISBuPNVSqVjOKNGuVBGJcK/YcNwhzNvnksvjpvflYYX98mTo1de\nLUpw/L0QFVKbOdOFuyohi8MB19ZvftOllYfvy2B4Niw43knHLS3wk58MzxisR6UBSBccERdWC7qc\nKMGZO9c9lFY2AJEdE5wYaj2GE/d0lCdpoBLB8bHqI4+s3TjOPfdkDw1lHcPxA9W+I4+6BlHzeYKC\nkxTHr2VILcnhpK2AWWkBzxdeGO5wxo1zf9eXXsp/LnCiEldGKE1wqkkamDDBtT3K4Rx5pLs2lUxA\nzDKGE3yfb34Tzj57+PYkh9PV5f52cXOfNm92Jal8h10vhzNvnmtXcP2cMOFxnEcegZNPHr6Pr2xf\ndFUSE5wYdu50f4BKO6k8WWpZQ2rbt7tON2kBpzB+NnVULLdSPvYxeDBj7l/WMZzgpE+IvgbPP+++\nPEGCT6VJYZValrdJczhJHXCl4zjhkBpUF1bzdenikjOS3IK/5mnLMUQh4u7HKIcjAqeeWlktwawh\nNc+ZZ7qxnCBJSQOQHFbbuRNefhm2bnWv6xlSS3o4gOGCs327C8svXTpyv3qE1UxwYti504Uwis5S\ny5M0sH27+5LnadOOHbV1OEND7mkurSigJ+sYTrjDCIcVBwfde4a/KMGkgaSQWr0cTloHXIng9Pe7\nzmzu3OHbqxEc73BmzBh5XbZuTZ6vNHq0+/GOLo/DAZf4smxZ9O8qLV6bNaSWRJLDgXTBAXj0Ufdv\nPQUnbf+g4Dz6KJx00vCyTB4TnAayc6frpCsRnKEhd1N5h5M3pBY33uDDaXnCal5wauVwXn3Vdap5\nBCduDCcYUgt3GOFr8MILbiwq/NSZJ6TW6Hk4UJngvPiiE5twx16p4AwOlku3RAnxli3DU2aj8J1z\nXocD8O53x4tUNYKTNaQWR7WCM3YsPPaYe10vwTn+eHjve5P3CQpO1PiNxwSngXiHU8lT8f79rmMJ\n1mGqVUht7NjKBKdWDsffkFnFK2kMJyqk5glfg2efhaOOGnmOcNJAUkitVR1OVDgN3NNtJYLjO7Wu\nruh7bevWdMHxnW8lgpPEqafCL3+ZPxkib0gtirFj3fsePJhfcHbtcm3PIjhJf/88tdTAZaH9n/+T\nvE8wacAEp0mpxuFEZbjUKqS2aFFjHc7mzS5mXOuQWjWC469tvZIGkhxOWtJAkuCowu///sgy+uEM\nNU+lDieYFFCtw6kkpJbEzJkunPf00/mOq0VILVhot5IxnO7u+ofUsrBwIfz6105MkwTHl8opEhOc\nCHydpUodTpTg1CpLbcmSygRn3jyXgVJtteJNm+BNb6qN4CSF1MIhsGeeqc7h1HMeTlIHnFTA84EH\nXEHO++4bvj2coeY54ojKOoi0CbZbtqTXnKsmpJZGJWG1WoTUoPy5KgmprVjh/la+JFazCM748c4J\nrV/vHjqPOy56P3M4DaKvz90Yhx1WWSe1f3/2wn9RIbXJk8tPj54DB9yX+4gjsgvO0JDrhKdPd2mT\n8+fnL40TZtMmFzrYvj1befy0eThxIbXJk9218dfg2WddaZcwvrMbGGj+eTiQ7HCuvtrV+3r44eHb\n40JqtXI44UXYGulwoDLBqUVIDaoTnMMOcw9FTz5ZmeAMDCTXsKuGxYvh9tvddyju/CY4DcI/lVea\nSuvXuPHkDal1dY18b5/efMgh8as0htm9253b5+jXYhxn82bX0S1YkH6uoaHoyZqepJBaV9fw38eF\n1ETKLifpKTdKxCuliDGcbdvgzjvh//7faMGJCqnNneuOy/uZkhzO0JCb29OKDqeRgrNrl7uWJ5/s\nxnEqmfjpHz6LWKN48WL47nfjw2lggtMwgoLTiJAajOwI/NodM2dmdzg+nOZZtKh6wdm0yXV0WdZL\n37PHXYe4J+CkkBqUw2B797pOMKrThbLgJD3lehEPL/pWCUWM4Xz1q/C+98Fb3+r+Rn5FTogPqY0Z\n40IlW7bka3/SGM4rrzjRTnvKLippAFza7nPPDb8GadQ6pLZ3b36HM326a/ujj1bmcIoIp3kWL3Zz\n58ITPoMcckhZbIvCBCcCf/NUGvfPs5ZG1OAkFCM4Rx5ZfeLA5s3uSShcLiOKpPEbGBlSC3cYXvDX\nr3fvFzeb2icOpHU6tQqrVTOGEyU4AwNw7bXw8Y+78xx/fHnweWDACYovYRKmkrCan/QJI++zLBlq\nUGxIbexYV/8v7PSSKCKkljVpwI/5TpvmBCfJ4TRScCDZ4Yi4h8kiXY4JTgThkFre+kJ5stSSHE5Q\n7FrR4SSN30A5zDU4GO9wdu6MD6d5sjgcf75aJA7Uegzn9ttdMsiJJ7rXr399ubPduNGFt+LOWUni\ngJ/0CSMnfmYZv4FiQ2qQL6ymWl5SoVoqCan5VW3HjXOC8/jjbhy3GQXnpJOS9ys6rGaCE4HvKMeM\ncTdI3gq2eUNqreJwfNXZWbOceKUJTtIcHHBhrsmTXQeYJDhxGWoen8qalDQA9XM4abXUwllqV18N\nFwUW6AgKTlw4zVOJw0kKqWXJUINiHQ7kE5y+Pvd9S6onlpVKBCfo5A85xN2D69c3l+Acd5xbMTYt\n7GiC0wCCN1AliQN5Q2pFjeGEF5mq1uFs2QKzZzuhqEVIDcqJAUljOFkdTlpIrVZzcaIczpgxlTmc\nTZvcvJtgpeGg4MQlDHgqDanFJQ3kcThFjeFAPsGpVTgNKhMcnzDgOekkJ+rNJDjTpsG//mv6fiY4\nDSAsOHk7qWDhTqg8pBYWnBkz8juc4Jofc+aUB+ErwY/fQDmklhRuzCo4u3ZFdxr+2selRHvqHVKr\nplp0lOAceeTwY4491oXJdu+OT4n2VFJtIOhwwtl7WQXHP0QV5XAWLXJiliVcWKsMNRieNJB1DCd8\nn/uwVd5KA0UKTlaKXojNBCeC4A1USSeVNaQ2NBT9JOXfN8rhzJhRLuKZRjik1tXl0pnj5uKsW5d8\nXj9+A04MxoxJLmeeNoYD6Q5nx470kFq9kwZqWS36pZdciDLI6NFuPGf16mJCasFr7dPP/T3eLGM4\nIu4hI8sE4yIEp9KQGpQzwVpRcIpeatoEJ4JqHU74Zh071olLeOEtX3MtKvYcJzjjx7sveJblbsOC\nA9HjOHv3ugypo46C//7v+PNt2lR2OJAeVksbw4FyqnKc4Dz3nLt2Sct8T5jgzjEwMNxZhqlVSK2W\n83Befjn6s/mwWpaQWlIHcccdI38fXu8meF3yZqkVJTjgHLkv959E2thdHioVnOBcsySHc9hh8WsY\n5a2jVgQWUmsAtXY4wRpNQZKeaMJCFwyPeZeTRpTgHHUUfPGLcP31TiwefdR1bjt2wCc/6TKm4ti8\neXiJ/LRMtTwhtTjBeegh96SbNBlu4kT3JZ46NXm/ZsxSi3I4MFxwkhzO4Yc7lxk1LjAwAP/rf8F/\n/ufw7Ul16/I6nKJCapBdcJrN4SxZ4kJTUclACxc61xoVSWgWh2OCU2dq7XAgOqwWl6EG8Q4Hso/j\nRAnOX/81fPSj8LOfudpPb387fP7zbgXED37QCU5cWC3scNIy1apNGpg2LT1hANy13rYt/Sm3Hllq\nWcZwgllqSQ7ngQfKlR3iGDXKdcxRq2TefffwRcE8cQ5HNXuWWpETPz2NEpy+vnwTP8NJA6NGuWKZ\nUeLhJ9VGfX+bQXDmznXXvNKly9MwwYmg1llqEJ2plmSh0wQnS3mbKMGZOtUJy7//u+tcNm+G885z\nv/uN33Btf+aZ6PNFOZykkFqWMRx/faPmUfi/QVbBSet0Kg2pfe1rcO+95dfVjOGEl5iOczhHH+3E\n4tBDR75XmLjEga9/HY45ZmQlgjiH09vrHGKWzrvZHE4tQ2q7drmxrbCQZnU4kOy0vcsJ0wyCM3as\n618qXbo8DROcCIJjD5WEYcJZahCdqZZ0gxXlcIJ0dY0M/Z19dnxYLWoMJ8nhZBnDmTrVdYjjxo3s\nuPIKTlqnU8nfsq8PPvtZV4fKU48xnFGj4JRTksNpnqjEge3bnUhefPFIwQk7HD/5M2s4DZpvDKdW\nDmfCBLfIYFQiTx7BSSIucWfbNveA0WiKTBwoXHBE5AwRWSsiz4rIJTH7XCUi60TkURE5Oe1YEZkh\nIveIyDMi8iMRmVbavlBE9orI6tLPNZW0uV4htbiyNjBccPxMfD8wWSvBieLss91AcxTBtGhIF5xt\n26Kf3oNMnepu7iix8H+DpJRoyO5wKvlb/r//5zqaxx8vb4tzOAMDtRvDARdWyyo44Q7i5pvhjDOi\nHU5cSK1VBafWIbVKBCeuQG0UcQ5n/fr4pbfrSZGp0YUKjoh0AVcDpwPHAeeKyDGhfc4ElqjqMuAC\n4NoMx34OuFdVjwbuAy4NnHK9qp5S+rkwb5tVh1c4rkXSAESH1LI6HH9D+3XIswhOcGmCPHR3u4mI\n27YN3757t7s2wS/2ggVOhMLZd+D2ffHF5PEHcJ9r06boDsP/DZYuTT5HMGkgibx/y8FB+Id/gH/+\nZ3jiCXdNId7hHDiQfx7OSy/FZ+B95CNwwQXp7TziCFcSP8jXvw4f+pATkHCnHRdSy5qhBuUHqHYM\nqaUJTniMM6/D8QuihVm/Pv1erwdFLsRWtMM5FVinqi+o6gBwM7AytM9K4EYAVX0AmCYis1OOXQl8\nvfT/rwOBedpUVdx7z55y6jHU1uFECU6cw5kyxf1+cHBkxYAsghNemiAr48bBu94FP/jB8O3e3QRj\n02PGuA4qavxgxw7XEaU9eU6dGi84M2e6UFZajawJE7IJTt6/5e23O/fxO7/jjvVPpXEOx9fUSorf\nBwVHNT6kBq6I52mnpbfzve+F+++Hv/ord841a9zf5Ld/2/19sjicHTvyORz/AFWkw5k1y12ftAHs\nWlcaePXV6O/l6NHuuoXvoXDSQBpRDqe3150nOEbaKFpZcOYBwe5oY2lbln2Sjp2tqtsAVHUrEAxK\nHFkKp/1ERN6at8Hhge5aJQ3EhdTiHE5wPZjg+A1kE5xKwmmelStHhtWCkz6DxIXVNm5MdzeQLDgi\nbsnlNCZMcE/aWUJqeYqx/t3fwZ//uWvHiSeWw2oHDkQLTl9f+tN+UHB6e93+cQ8dWZk7F/7rv+CH\nP4Q/+zOX5HDeee7cU6e6a+NL/fv3DrY/GFLLkqEG9UkaGDPGte3VV5P3q3VIbfv2aIcD0WnDlTic\nsOA895z7LnU1wah6kWM4Bd0qVVGJQ/FdyBZggaruEJFTgNtEZLmqjlhZY9WqVa/9v7u7m+7ubmDk\nzdOokJp/75076y84Z57pOq7gGFM4YcCzaFF0plqWcBo4ERgYqK7D8Nc6zeGMGeP2zRKC+cUvXFjR\n1zg78URXdn7lyujS815w0p72g8U7X345fYwrK7NmwU9+Ar/3e27ezRNPuO0iZZezbNlIdwPl+0wk\nfvnhMF5wRo0qzuFAOayWdJ1qHVIbGooXHP/0f/zx5W21SBpolvEbGO5wenp66Onpqdm5ixacTcCC\nwOv5pW3hfY6I2GdswrFbRWS2qm4TkTnASwCq2g/0l/6/WkSeA44CVocbFhScIOGbpxa11CB/SA3y\nCc7tt7sBZj/LOXxMHmbOdAPWP/6x62BhZEq0Z/Fi93QWJqvg+I6iFoKT5Rxz57rPktZB/d3fwac+\nVQ5JnnhiOVMtzuHs3Zsu8kGHkzR+UwlTprhVQ++8c3iH6Mdxli1Lrsq9f3/+pIFg+LkIvOD4pRui\nePXV/GOVcfjvY5rgBMmbNDBrlhP+4APnunXNMX4Dw5MGgg/jAFdeeWVV5y7awD0ELC1lj40FzgHC\nOVB3AB8EEJEVwM5SuCzp2DuA80v//xBwe+n4Q0vJBojIYmApkKEaU5miHE7ekJp/7507RxbhjBKc\nf/onuPHG8utqHA7A+9/vljv24ac4h7NkSbzgxC0aFqQWguM7iSxPuVnCBUNDbtLkBz5Q3hYMqcU5\nHEjvfIOCU0uHEzx/OAwZHMdJcjh5xnBGj3ZinCWMWA1z5iSvaDo0BGvXumy8WpBXcA4ccGNMcftH\n0dU1MpW9WRIGoPwdybsOWBYKFRxVHQQuAu4BngJuVtU1InKBiHystM9dwK9EZD1wHXBh0rGlU38Z\n+G0ReQZ4B/Cl0vbTgMdFZDVwC3CBqubyJ2HBmTy5PHiflXqH1FRduOcXvyhvq1ZwPvhB92T5wx+6\n13EOJ05wso7h+CfDeoTUINuA6ObNrl3Bp9ajj3bH9fXFOxzIN4ZTa4cTR7DTTnI4ebLUoOxy6uFw\n4nj+ebcGTR6HkYQXnLjIQ/j+8QkDSYkiUYTHcZoppDZpkvtOZa1Kn4fCx3BU9W7g6NC260KvLyKC\nqGNL27cD74zYfitwazXtDQtOcPA+awce53DClZX7+pI75aDgBOdjhGupbdniBPHxx8vvXa3gjB4N\nX/oSXHIJnH56vMNZutQJjurwL13WkNqkSdlnt8eRJ6SWpVbUunUjv/yjR7un6KeeinY4vtNttMOJ\nIs3hzJjh2gL5wrATJ7r7s2iHk/SA8PjjyeG2vPh7Kcnh3BroYfKO33jC4zjN5HCg7HIOOaS2522C\nnIjmIuoGypupVquQmh8/CjucyZNdp+c7rsceg9e9DpYvLy/cVa3ggJsEOm0afOMb8Q5nxgwXWgmL\naVbBEXGC3kwOJ+7L75cMEBmZbt7V5TrePIJTL4cTnIsTXF7aM2mSe2CZNStflpR3AY10OLUWnDFj\nysklUYTvn0oFJ+hw+vrcdzxLCLpeFJUabYITIk5w8iQOFB1SExleT+2xx1yywFveUg6r1UJwROBv\n/xYuv9xlbMXNEQiH1VTdzZr1C1QrwamVw0kSnIcfji47D257o7LUkgg6nN7ekQ5HxN1recJpUBac\noh1OPQUH3OfKIziVhPOCkz+fe85lezZDSrSnqGoDTfQRm4MowUlLHNi9u/z/oaHoiYFRWWpJpW38\n+3rBCYtHcBwnKDh+PZtaCA7Am9/sMtamTYvvaH1YzeNnamctRDhtWv2SBrI8uUWF1MB1bA89FF9M\nM4vgNMrhJIXUoDLB8ROL845f5CFNcJ54ohjBiftezpjhvt+9ve51LRxOM43feMzh1ImogpNJDmfD\nBtfh+owOnxId/hLGLU9QicOBaMF585udw1GtneAAfPnL8OEPx/9+yRL3pfFkDad5pk6tbh5F3pBa\nNQ7nySeTHU6epIF6OZy0pAGo3OEUGU6DZMHZs8f9LWvdWSc5HJHh91DeKgOeoOA0U0q0p6jJnyY4\nIVgwydIAAA1hSURBVPI6nPvvdx2HX48kbsnouJBaJfNwoCw4+/Y50TvmGHeTTJ7s1pCppeAsW+ZE\nJ45wSC2v4PzFX8Ab31h5+/KE1GbNctcmbplfVfdZojqAWbOcI6nG4fiq0qr1cziHHebuo4GB2jqc\niROLDaeBu4f7+oavIeR56il339e6DUmCA8Of/it1OPPmuYeAgwebL2EAzOHUjag1XJKSBh580P37\n9NPu3zjBiQuppTmcHTuixcMLzlNPufL9/qnbh9VqKThphENqecZvwE0uraYs++jRcN996evGgIuT\nR9UX82ze7IQrTrxOOqm6MRy/zsqBA8l11GrJqFHufbZtaz2H09UFs2ePLCYLxYzfQH0EZ+xY9wCz\nebOF1DqavEkDfgnkLIJTSUht40YXogt3cl5wfDjN48Nq9RScakNqteDtb8++b1K4IO1p88QTq3M4\n4I5/+WX3dw1XpCgKL7JxDufyy7PVrQsyaVLxggPRFa+heQSn0jlAPqxmDqeDyRNSO3gQHnnETZJM\nE5xKQ2q//nX03Ig4wXnLW1whx0qWJqiUww93T85+ILURgpOHpHGcLIJTzRgOOMF58cX6jN94/FhI\nnMM58cT8LrMeITWIH8cpImEA6uNwwAnO2rXu4aPZvi/TppWXOKklJjgB/AUODz7HOZynnnI3ypve\nVBac/ftrG1IbHMwnOMcf7zrTSpYmqBSR4VWjm11wkhxOXIaa521vc8VNo8jqcMaPd9eoHuE0T5rD\nqYR6hNQgWnBUncM54YTav99ll8E73hH/+6DgVJo0AG7y5333uUnd9fquZkUk2xSCvJjgBNizJ/qp\nLc7hPPggnHqqm3D51FPuS7BvX3SYZNIk93Tps9lU09Oip04tz7kJM3OmSz9+/PHhgjN6NKxYUXnh\nzkpZurQcVss7hlNvksIFaQ5nwQL4m7+J/l2ekFq9HU5QcGpZyr9RDmfTJnetZ8+u/futWJE8w76W\nDue++5pv/MZTRFjNBCdA3M0TlzTw0EPwhje4jkPEZR3FhdSmTHGi4/+A+/a5DirpycaX1YkSjxkz\nnLuZOHFkKOQtb6nf+I3HZ6oNDbnOoJkFJ+nJrZp4up+lnsa4ce4+aITDiZr4WSmNdDhFjd9koZaC\n89JLzTd+4zHBKZgkwYkKqXmHI+Jczpo18YIj4srP/PKX7nVaOM0zfXq0eMycCc88M9zdeN71Ljj2\n2PRz1xIvOC+/7MQ1T/XcehP3RVKtTnCa2eH4TruWDqdeSQNRglPU+E0WDj3UCfe+fdUnDYAJTscS\nJzhRIbW9e918F9/hH3usG8eJExxwM/Z9rbO0DLXge8eF1CBacN78ZrjppvRz1xIfUmv28RuIdzhb\nt7qn9ko7kDxJA+3icBoVUitq/CYLXV3le6gah7OgtNpXs4bUbAynYPI4nEcecQP0PkV2+fJ0wQk7\nnCxLC1ciOI3AO5xmH78BVxNuyxYX/gtSbXpqMzucdksaaGRIDdw9vmGDSxKq9HpOmeLGoI4eUQ+/\nOTCHUzB5HM6DD7rxG48XnLgsNXAO55e/dKGbrA5n2rRowZk2zYXpmkVwFixwHdpzzzW/wxk3zv1N\nX3pp+Pa0DLU08mSpbd1aX4czZ46bPJk1lJuFejmc2bPd9fIJNwcOuIeD5cuLf+845s93iUL+e1gp\nTz5ZDq01GyY4BRMnOOPHu5s9WF7Dj994gg4nbjLf3LnOjr/4YvocHM9HPxo9qXHUKLci51FHpZ+j\nHowZ44Tm5z9vfsGB6C9TPR2Oan0dzvjxTmjGj69dCu6cOfXJhvRjRX5OyI03ulJI9Zo0G8X8+U4s\nqp3rVk2FjaIxwSkQVfjRj+LtbdjlhAVn3jz39LhpU3LhP+9ysj5p/s7vuHBVFBdf3Fz5+0uWwM9+\n1jqCE45P10Jwso7hQH0dDjiBqFXCALh7+fvfr935kvBhtW3b4POfd0uqNxIvOLVaabQZCSZH1AoT\nnBLf+x786lfOUUQRTI1+9VW34FhQnERc4sAvf5mcoeXHcbKG1FqJJUtcSZ1mH8OB6Mmf9QqpNUpw\nDj+8duM39cZXvP70p+EjH2lcwoDHh9TqVc2jEXR1uaiML0xck3PW7lSthS/D4v//yU/CtdfGly2Z\nPRve/374zGfg7//eCUd4waTly10yQZrgPPxw9pBaK+GdWCs6HJ8SHecms5BHcJLWFyqKww+vrcOp\nJ3PmuFDa//yPq/vWaObPd/1GOwsO1D6s1rGC87a3lZV71Sp45zvhtNPi97/zTjfD/JBD3HybP/7j\nkfssX+6ygJIEx4fU2tHh+HDUvHmNbUcWwg7npZecEFQzYTaP4NTb3UDrO5x/+zf4539ujgc17+Lb\nXXD+4R/guONqd7465Jg0J3/4h64G2he/CP/+7y4em8TUqU6U3vnO+H181kyS4Myd6+L8a9e2n+As\nW+Y6hizLBDSasMN57LHq50MsX57ts48fX9+EAU+tx3DqydFHwwc+EF/Hrt7Mnu3GT9t5DAeGZ+LW\ngo4VnL/8S5eOeP75cM01tXni9IKTlj3z+te7wfXf/d3q37OZOPZYV6m6FQg6HFW44gr42MeqO+f5\n52fbr1EOZ+nS9AerZuXCC91PszBqlHt4bHeHU2sKD6mJyBkislZEnhWRS2L2uUpE1onIoyJyctqx\nIjJDRO4RkWdE5EciMi3wu0tL51ojIu9Katt557ny/3/yJ7X4pE7AJkxIL+vyute5iWvt5nBEqhsD\nqSc+Nq0KN9/s5nZ86EP1ee9x4xrjcM4+G264of7vWwtEqpvvUgTz55vg5KVQwRGRLuBq4HTgOOBc\nETkmtM+ZwBJVXQZcAFyb4djPAfeq6tHAfcClpWOWA+8DjgXOBK4RSb5N58yp3Y3c1eWe8tNizK9/\nvevomkFwenp6Gt2EhjBlintK3boVPvc5N6fpZz/rqct7n3CCKz9Ub0RGJrrE0an3RRRx1+LIIxvj\nVFuZoh3OqcA6VX1BVQeAm4GVoX1WAjcCqOoDwDQRmZ1y7Erg66X/fx14T+n/ZwM3q+pBVd0ArCud\np278x38kJx+AczjQHIOfndyxzJ8Pn/qUm0912mn1uxZ/8AfZw2+NopPvizBx1+Jf/gXe+976tqXV\nKXoMZx7wYuD1RkYKQNQ+81KOna2q2wBUdauI+ADFPOB/AsdsKm2rG1lm/h9+uPtpBofTycyb5+Zf\nrVnT6JYYrUi7JwwUQTMmDVQS4NKat6Jgzj67eWsodQrHHONKpCxe3OiWGEaHoKqF/QArgLsDrz8H\nXBLa51rgjwKv1wKzk44F1uBcDsAcYE3U+YG7gTdGtEvtx37sx37sJ/9PNZpQtMN5CFgqIguBLcA5\nwLmhfe4APg58S0RWADtVdZuIvJJw7B3A+cCXgQ8Btwe23yQiX8GF0pYCD4YbpapNlu9iGIbR/hQq\nOKo6KCIXAffgEhRuUNU1InKB+7Ver6p3ichZIrIe6AM+nHRs6dRfBm4RkY8AL+Ay01DVp0XkFuBp\nYAC4UNUXNTcMwzAaiVh/bBiGYdSDjqullmUiarsiIvNF5D4ReUpEnhCRi0vbYyfStjMi0iUiq0Xk\njtLrjrwOACIyTUS+XZow/ZSIvLETr4eIfEpEnhSRx0XkJhEZ20nXQURuEJFtIvJ4YFtNJtpDhwlO\nlomobc5B4NOqehzwJuDjpc8fOZG2A/gkLvzq6dTrAPCPwF2qeixwEi55p6Ouh4jMBT4BnKKqJ+KG\nHM6ls67Dv+H6xyA1m2jfUYJDtomobYuqblXVR0v/34PL9ptP/ETatkVE5gNnAf8a2Nxx1wFARKYC\nv6mq/wZQmji9i868HqOASSIyGpiAm8vXMddBVf8L2BHaXLOJ9p0mOHGTTDsOETkSOBm4n9BEWqAB\nlb7qzleAz+JSPT2deB0AFgGviMi/lUKM14vIRDrseqjqZuDvgV/jhGaXqt5Lh12HCGbFfP5wf5o6\n0b7TBMcARGQy8B3gkyWnE84caetMEhF5N7Ct5PaSQgBtfR0CjAZOAf5ZVU/BZYt+js67L6bjnuYX\nAnNxTuc8Ouw6ZKDiz99pgrMJWBB4Pb+0rWMohQq+A3xDVf38pW2l+nWIyBzgpUa1r068BThbRJ4H\n/gP4LRH5BrC1w66DZyPwoqo+XHr9XZwAddp98U7geVXdrqqDwPeAN9N51yFM3OffBATX903tTztN\ncF6biCoiY3GTSe9ocJvqzdeAp1X1HwPb/ERaGD6Rti1R1b9U1QWquhh3D9ynqh8Avk8HXQdPKVzy\nooj4SoDvAJ6iw+4LXChthYiMLw1+vwOXVNJp10EY7vzjPv8dwDmlTL5FxEy0H3biTpuHIyJn4DJy\n/GTSLzW4SXVDRN4C/Ax4gnKpir/E3SS34J5WXgDep6o7G9XOeiIibwM+o6pni8hMOvc6nIRLoBgD\nPI+bgD2KDrseInIF7iFkAHgE+F/AFDrkOojIN4Fu4BBgG3AFcBvwbSI+v4hcCvwJ7np9UlXvSTx/\npwmOYRiG0Rg6LaRmGIZhNAgTHMMwDKMumOAYhmEYdcEExzAMw6gLJjiGYRhGXTDBMQzDMOqCCY5h\nGIZRF0xwDMMwjLrw/wNaBDZquMasYwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x147f8a110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.plot(dfsMLP.test_Accs)\n",
    "# plt.plot(dfsMLP.losses)\n",
    "plt.plot(abs(dfsMLP.selected_ws[0]))\n",
    "plt.ylabel('some numbers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(abs(dfsMLP.selected_ws[0])[abs(dfsMLP.selected_ws[0])>0.001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "data = [\n",
    "    go.Heatmap(\n",
    "        z=[[1, 20, 30],\n",
    "           [20, 1, 60],\n",
    "           [30, 60, 1]]\n",
    "    )\n",
    "]\n",
    "\n",
    "py.iplot(data, filename='basic-heatmap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                       Output Shape        Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_1 (Dense)                    (None, 300)         150300      dense_input_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)          (None, 300)         0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                    (None, 2)           602         activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)          (None, 2)           0           dense_2[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 150902\n",
      "____________________________________________________________________________________________________\n",
      "Train on 8547 samples, validate on 2137 samples\n",
      "Epoch 1/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.7727 - acc: 0.4363 - val_loss: 0.6933 - val_acc: 0.5662\n",
      "Epoch 2/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.7095 - acc: 0.5713 - val_loss: 0.7237 - val_acc: 0.5662\n",
      "Epoch 3/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.7032 - acc: 0.5713 - val_loss: 0.6843 - val_acc: 0.5662\n",
      "Epoch 4/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6867 - acc: 0.5670 - val_loss: 0.6971 - val_acc: 0.4338\n",
      "Epoch 5/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6926 - acc: 0.4987 - val_loss: 0.6837 - val_acc: 0.5662\n",
      "Epoch 6/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6837 - acc: 0.5713 - val_loss: 0.6908 - val_acc: 0.5662\n",
      "Epoch 7/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6867 - acc: 0.5713 - val_loss: 0.6836 - val_acc: 0.5662\n",
      "Epoch 8/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6814 - acc: 0.5713 - val_loss: 0.6835 - val_acc: 0.5662\n",
      "Epoch 9/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6823 - acc: 0.5713 - val_loss: 0.6818 - val_acc: 0.5662\n",
      "Epoch 10/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6805 - acc: 0.5713 - val_loss: 0.6828 - val_acc: 0.5662\n",
      "Epoch 11/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6804 - acc: 0.5713 - val_loss: 0.6809 - val_acc: 0.5662\n",
      "Epoch 12/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6791 - acc: 0.5713 - val_loss: 0.6803 - val_acc: 0.5662\n",
      "Epoch 13/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6788 - acc: 0.5713 - val_loss: 0.6797 - val_acc: 0.5662\n",
      "Epoch 14/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6782 - acc: 0.5713 - val_loss: 0.6796 - val_acc: 0.5662\n",
      "Epoch 15/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6777 - acc: 0.5713 - val_loss: 0.6785 - val_acc: 0.5662\n",
      "Epoch 16/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6769 - acc: 0.5713 - val_loss: 0.6781 - val_acc: 0.5662\n",
      "Epoch 17/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6764 - acc: 0.5713 - val_loss: 0.6774 - val_acc: 0.5662\n",
      "Epoch 18/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6762 - acc: 0.5713 - val_loss: 0.6768 - val_acc: 0.5662\n",
      "Epoch 19/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6755 - acc: 0.5713 - val_loss: 0.6771 - val_acc: 0.5662\n",
      "Epoch 20/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6748 - acc: 0.5713 - val_loss: 0.6756 - val_acc: 0.5662\n",
      "Epoch 21/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6745 - acc: 0.5713 - val_loss: 0.6751 - val_acc: 0.5662\n",
      "Epoch 22/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6735 - acc: 0.5713 - val_loss: 0.6747 - val_acc: 0.5662\n",
      "Epoch 23/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6730 - acc: 0.5713 - val_loss: 0.6738 - val_acc: 0.5662\n",
      "Epoch 24/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6725 - acc: 0.5713 - val_loss: 0.6731 - val_acc: 0.5662\n",
      "Epoch 25/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6714 - acc: 0.5713 - val_loss: 0.6731 - val_acc: 0.5662\n",
      "Epoch 26/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6712 - acc: 0.5713 - val_loss: 0.6720 - val_acc: 0.5662\n",
      "Epoch 27/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6702 - acc: 0.5713 - val_loss: 0.6710 - val_acc: 0.5662\n",
      "Epoch 28/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6697 - acc: 0.5713 - val_loss: 0.6705 - val_acc: 0.5662\n",
      "Epoch 29/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6690 - acc: 0.5713 - val_loss: 0.6699 - val_acc: 0.5662\n",
      "Epoch 30/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6682 - acc: 0.5713 - val_loss: 0.6690 - val_acc: 0.5662\n",
      "Epoch 31/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6674 - acc: 0.5713 - val_loss: 0.6683 - val_acc: 0.5662\n",
      "Epoch 32/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6668 - acc: 0.5713 - val_loss: 0.6677 - val_acc: 0.5662\n",
      "Epoch 33/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6661 - acc: 0.5713 - val_loss: 0.6670 - val_acc: 0.5662\n",
      "Epoch 34/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6651 - acc: 0.5713 - val_loss: 0.6662 - val_acc: 0.5662\n",
      "Epoch 35/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6650 - acc: 0.5713 - val_loss: 0.6653 - val_acc: 0.5662\n",
      "Epoch 36/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6641 - acc: 0.5713 - val_loss: 0.6657 - val_acc: 0.5662\n",
      "Epoch 37/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6629 - acc: 0.5713 - val_loss: 0.6640 - val_acc: 0.5662\n",
      "Epoch 38/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6632 - acc: 0.5713 - val_loss: 0.6629 - val_acc: 0.5662\n",
      "Epoch 39/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6616 - acc: 0.5713 - val_loss: 0.6636 - val_acc: 0.5662\n",
      "Epoch 40/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6610 - acc: 0.5713 - val_loss: 0.6613 - val_acc: 0.5662\n",
      "Epoch 41/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6600 - acc: 0.5713 - val_loss: 0.6605 - val_acc: 0.5662\n",
      "Epoch 42/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6589 - acc: 0.5713 - val_loss: 0.6602 - val_acc: 0.5662\n",
      "Epoch 43/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6584 - acc: 0.5713 - val_loss: 0.6590 - val_acc: 0.5662\n",
      "Epoch 44/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6574 - acc: 0.5713 - val_loss: 0.6580 - val_acc: 0.5662\n",
      "Epoch 45/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6565 - acc: 0.5713 - val_loss: 0.6574 - val_acc: 0.5662\n",
      "Epoch 46/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6555 - acc: 0.5714 - val_loss: 0.6564 - val_acc: 0.5662\n",
      "Epoch 47/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6550 - acc: 0.5713 - val_loss: 0.6555 - val_acc: 0.5662\n",
      "Epoch 48/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6537 - acc: 0.5713 - val_loss: 0.6544 - val_acc: 0.5662\n",
      "Epoch 49/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6530 - acc: 0.5714 - val_loss: 0.6535 - val_acc: 0.5662\n",
      "Epoch 50/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6525 - acc: 0.5713 - val_loss: 0.6527 - val_acc: 0.5662\n",
      "Epoch 51/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6509 - acc: 0.5713 - val_loss: 0.6515 - val_acc: 0.5676\n",
      "Epoch 52/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6501 - acc: 0.5714 - val_loss: 0.6506 - val_acc: 0.5662\n",
      "Epoch 53/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6491 - acc: 0.5715 - val_loss: 0.6495 - val_acc: 0.5676\n",
      "Epoch 54/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6482 - acc: 0.5714 - val_loss: 0.6485 - val_acc: 0.5676\n",
      "Epoch 55/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6473 - acc: 0.5770 - val_loss: 0.6475 - val_acc: 0.5686\n",
      "Epoch 56/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6461 - acc: 0.5722 - val_loss: 0.6471 - val_acc: 0.5662\n",
      "Epoch 57/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6453 - acc: 0.5714 - val_loss: 0.6456 - val_acc: 0.5854\n",
      "Epoch 58/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6450 - acc: 0.6237 - val_loss: 0.6443 - val_acc: 0.5751\n",
      "Epoch 59/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6429 - acc: 0.5754 - val_loss: 0.6442 - val_acc: 0.5662\n",
      "Epoch 60/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6421 - acc: 0.5718 - val_loss: 0.6421 - val_acc: 0.5723\n",
      "Epoch 61/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6407 - acc: 0.5838 - val_loss: 0.6409 - val_acc: 0.5723\n",
      "Epoch 62/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6395 - acc: 0.5747 - val_loss: 0.6398 - val_acc: 0.5718\n",
      "Epoch 63/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6385 - acc: 0.5939 - val_loss: 0.6386 - val_acc: 0.5910\n",
      "Epoch 64/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6374 - acc: 0.5801 - val_loss: 0.6374 - val_acc: 0.5742\n",
      "Epoch 65/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6360 - acc: 0.5918 - val_loss: 0.6361 - val_acc: 0.5877\n",
      "Epoch 66/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6347 - acc: 0.5875 - val_loss: 0.6349 - val_acc: 0.5845\n",
      "Epoch 67/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6337 - acc: 0.6130 - val_loss: 0.6337 - val_acc: 0.5812\n",
      "Epoch 68/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6325 - acc: 0.5772 - val_loss: 0.6325 - val_acc: 0.5765\n",
      "Epoch 69/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6310 - acc: 0.6189 - val_loss: 0.6311 - val_acc: 0.6472\n",
      "Epoch 70/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6297 - acc: 0.6330 - val_loss: 0.6300 - val_acc: 0.5784\n",
      "Epoch 71/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6282 - acc: 0.5894 - val_loss: 0.6282 - val_acc: 0.6097\n",
      "Epoch 72/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6267 - acc: 0.6283 - val_loss: 0.6268 - val_acc: 0.6088\n",
      "Epoch 73/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6259 - acc: 0.5968 - val_loss: 0.6255 - val_acc: 0.6074\n",
      "Epoch 74/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6239 - acc: 0.6132 - val_loss: 0.6239 - val_acc: 0.6233\n",
      "Epoch 75/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6224 - acc: 0.6299 - val_loss: 0.6224 - val_acc: 0.6256\n",
      "Epoch 76/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6210 - acc: 0.6481 - val_loss: 0.6213 - val_acc: 0.6083\n",
      "Epoch 77/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6202 - acc: 0.5945 - val_loss: 0.6194 - val_acc: 0.6308\n",
      "Epoch 78/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6178 - acc: 0.6694 - val_loss: 0.6177 - val_acc: 0.6565\n",
      "Epoch 79/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6164 - acc: 0.6293 - val_loss: 0.6166 - val_acc: 0.6177\n",
      "Epoch 80/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6151 - acc: 0.6753 - val_loss: 0.6145 - val_acc: 0.6949\n",
      "Epoch 81/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6131 - acc: 0.6650 - val_loss: 0.6132 - val_acc: 0.6294\n",
      "Epoch 82/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6114 - acc: 0.6392 - val_loss: 0.6112 - val_acc: 0.7113\n",
      "Epoch 83/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6099 - acc: 0.7139 - val_loss: 0.6094 - val_acc: 0.6635\n",
      "Epoch 84/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6078 - acc: 0.6738 - val_loss: 0.6079 - val_acc: 0.6500\n",
      "Epoch 85/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6063 - acc: 0.6511 - val_loss: 0.6058 - val_acc: 0.7000\n",
      "Epoch 86/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6043 - acc: 0.7016 - val_loss: 0.6040 - val_acc: 0.7160\n",
      "Epoch 87/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6026 - acc: 0.7011 - val_loss: 0.6022 - val_acc: 0.6991\n",
      "Epoch 88/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6006 - acc: 0.7042 - val_loss: 0.6003 - val_acc: 0.7033\n",
      "Epoch 89/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.5988 - acc: 0.6953 - val_loss: 0.5984 - val_acc: 0.6940\n",
      "Epoch 90/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.5968 - acc: 0.6944 - val_loss: 0.5964 - val_acc: 0.7478\n",
      "Epoch 91/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.5953 - acc: 0.7624 - val_loss: 0.5946 - val_acc: 0.6986\n",
      "Epoch 92/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.5932 - acc: 0.6808 - val_loss: 0.5924 - val_acc: 0.7679\n",
      "Epoch 93/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.5919 - acc: 0.8016 - val_loss: 0.5905 - val_acc: 0.7080\n",
      "Epoch 94/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.5901 - acc: 0.6660 - val_loss: 0.5882 - val_acc: 0.7263\n",
      "Epoch 95/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.5871 - acc: 0.7771 - val_loss: 0.5860 - val_acc: 0.7506\n",
      "Epoch 96/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.5856 - acc: 0.6957 - val_loss: 0.5841 - val_acc: 0.7277\n",
      "Epoch 97/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.5826 - acc: 0.7786 - val_loss: 0.5818 - val_acc: 0.7721\n",
      "Epoch 98/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.5807 - acc: 0.7284 - val_loss: 0.5804 - val_acc: 0.7160\n",
      "Epoch 99/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.5780 - acc: 0.7487 - val_loss: 0.5777 - val_acc: 0.8016\n",
      "Epoch 100/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.5764 - acc: 0.7927 - val_loss: 0.5755 - val_acc: 0.7412\n",
      "Epoch 101/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.5737 - acc: 0.7610 - val_loss: 0.5729 - val_acc: 0.7763\n",
      "Epoch 102/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.5714 - acc: 0.7656 - val_loss: 0.5706 - val_acc: 0.7679\n",
      "Epoch 103/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.5692 - acc: 0.7898 - val_loss: 0.5683 - val_acc: 0.7740\n",
      "Epoch 104/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.5668 - acc: 0.7665 - val_loss: 0.5660 - val_acc: 0.7763\n",
      "Epoch 105/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.5643 - acc: 0.7788 - val_loss: 0.5636 - val_acc: 0.7978\n",
      "Epoch 106/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.5622 - acc: 0.8051 - val_loss: 0.5613 - val_acc: 0.7707\n",
      "Epoch 107/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.5597 - acc: 0.7824 - val_loss: 0.5587 - val_acc: 0.7871\n",
      "Epoch 108/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.5573 - acc: 0.7754 - val_loss: 0.5563 - val_acc: 0.7908\n",
      "Epoch 109/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.5547 - acc: 0.8052 - val_loss: 0.5538 - val_acc: 0.7960\n",
      "Epoch 110/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.5523 - acc: 0.7894 - val_loss: 0.5512 - val_acc: 0.8025\n",
      "Epoch 111/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.5498 - acc: 0.8146 - val_loss: 0.5490 - val_acc: 0.7885\n",
      "Epoch 112/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.5472 - acc: 0.7903 - val_loss: 0.5462 - val_acc: 0.8011\n",
      "Epoch 113/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.5448 - acc: 0.8206 - val_loss: 0.5443 - val_acc: 0.7791\n",
      "Epoch 114/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.5430 - acc: 0.7673 - val_loss: 0.5411 - val_acc: 0.8222\n",
      "Epoch 115/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.5399 - acc: 0.8387 - val_loss: 0.5386 - val_acc: 0.8002\n",
      "Epoch 116/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.5376 - acc: 0.7889 - val_loss: 0.5359 - val_acc: 0.8273\n",
      "Epoch 117/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.5347 - acc: 0.8350 - val_loss: 0.5333 - val_acc: 0.8035\n",
      "Epoch 118/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.5316 - acc: 0.8085 - val_loss: 0.5305 - val_acc: 0.8152\n",
      "Epoch 119/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.5289 - acc: 0.8223 - val_loss: 0.5279 - val_acc: 0.8170\n",
      "Epoch 120/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.5263 - acc: 0.8192 - val_loss: 0.5252 - val_acc: 0.8283\n",
      "Epoch 121/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.5242 - acc: 0.8367 - val_loss: 0.5226 - val_acc: 0.8184\n",
      "Epoch 122/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.5210 - acc: 0.8323 - val_loss: 0.5204 - val_acc: 0.8063\n",
      "Epoch 123/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.5187 - acc: 0.8093 - val_loss: 0.5172 - val_acc: 0.8329\n",
      "Epoch 124/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.5157 - acc: 0.8374 - val_loss: 0.5146 - val_acc: 0.8194\n",
      "Epoch 125/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.5129 - acc: 0.8234 - val_loss: 0.5118 - val_acc: 0.8311\n",
      "Epoch 126/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.5104 - acc: 0.8422 - val_loss: 0.5094 - val_acc: 0.8180\n",
      "Epoch 127/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.5077 - acc: 0.8219 - val_loss: 0.5064 - val_acc: 0.8428\n",
      "Epoch 128/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.5049 - acc: 0.8479 - val_loss: 0.5047 - val_acc: 0.8105\n",
      "Epoch 129/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.5032 - acc: 0.8146 - val_loss: 0.5010 - val_acc: 0.8414\n",
      "Epoch 130/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.4993 - acc: 0.8416 - val_loss: 0.4988 - val_acc: 0.8184\n",
      "Epoch 131/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.4966 - acc: 0.8289 - val_loss: 0.4962 - val_acc: 0.8587\n",
      "Epoch 132/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.4956 - acc: 0.8562 - val_loss: 0.4938 - val_acc: 0.8194\n",
      "Epoch 133/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.4915 - acc: 0.8330 - val_loss: 0.4907 - val_acc: 0.8582\n",
      "Epoch 134/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.4893 - acc: 0.8504 - val_loss: 0.4881 - val_acc: 0.8283\n",
      "Epoch 135/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.4861 - acc: 0.8403 - val_loss: 0.4850 - val_acc: 0.8460\n",
      "Epoch 136/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.4833 - acc: 0.8452 - val_loss: 0.4825 - val_acc: 0.8386\n",
      "Epoch 137/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.4808 - acc: 0.8505 - val_loss: 0.4799 - val_acc: 0.8409\n",
      "Epoch 138/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.4782 - acc: 0.8408 - val_loss: 0.4772 - val_acc: 0.8423\n",
      "Epoch 139/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.4754 - acc: 0.8452 - val_loss: 0.4745 - val_acc: 0.8456\n",
      "Epoch 140/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.4728 - acc: 0.8487 - val_loss: 0.4719 - val_acc: 0.8460\n",
      "Epoch 141/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.4704 - acc: 0.8461 - val_loss: 0.4693 - val_acc: 0.8573\n",
      "Epoch 142/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.4678 - acc: 0.8557 - val_loss: 0.4670 - val_acc: 0.8428\n",
      "Epoch 143/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.4651 - acc: 0.8492 - val_loss: 0.4642 - val_acc: 0.8596\n",
      "Epoch 144/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.4629 - acc: 0.8525 - val_loss: 0.4619 - val_acc: 0.8465\n",
      "Epoch 145/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.4598 - acc: 0.8543 - val_loss: 0.4591 - val_acc: 0.8591\n",
      "Epoch 146/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.4575 - acc: 0.8542 - val_loss: 0.4566 - val_acc: 0.8521\n",
      "Epoch 147/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.4550 - acc: 0.8614 - val_loss: 0.4548 - val_acc: 0.8423\n",
      "Epoch 148/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.4529 - acc: 0.8460 - val_loss: 0.4516 - val_acc: 0.8577\n",
      "Epoch 149/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.4499 - acc: 0.8616 - val_loss: 0.4498 - val_acc: 0.8442\n",
      "Epoch 150/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.4481 - acc: 0.8480 - val_loss: 0.4467 - val_acc: 0.8638\n",
      "Epoch 151/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.4450 - acc: 0.8618 - val_loss: 0.4447 - val_acc: 0.8507\n",
      "Epoch 152/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.4427 - acc: 0.8608 - val_loss: 0.4419 - val_acc: 0.8648\n",
      "Epoch 153/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.4401 - acc: 0.8647 - val_loss: 0.4404 - val_acc: 0.8484\n",
      "Epoch 154/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.4382 - acc: 0.8508 - val_loss: 0.4373 - val_acc: 0.8671\n",
      "Epoch 155/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.4359 - acc: 0.8679 - val_loss: 0.4353 - val_acc: 0.8526\n",
      "Epoch 156/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.4331 - acc: 0.8615 - val_loss: 0.4325 - val_acc: 0.8671\n",
      "Epoch 157/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.4311 - acc: 0.8605 - val_loss: 0.4301 - val_acc: 0.8662\n",
      "Epoch 158/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.4288 - acc: 0.8704 - val_loss: 0.4285 - val_acc: 0.8535\n",
      "Epoch 159/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.4265 - acc: 0.8545 - val_loss: 0.4259 - val_acc: 0.8685\n",
      "Epoch 160/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.4250 - acc: 0.8720 - val_loss: 0.4256 - val_acc: 0.8446\n",
      "Epoch 161/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.4235 - acc: 0.8491 - val_loss: 0.4222 - val_acc: 0.8699\n",
      "Epoch 162/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.4206 - acc: 0.8718 - val_loss: 0.4201 - val_acc: 0.8531\n",
      "Epoch 163/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.4178 - acc: 0.8640 - val_loss: 0.4169 - val_acc: 0.8648\n",
      "Epoch 164/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.4154 - acc: 0.8630 - val_loss: 0.4149 - val_acc: 0.8648\n",
      "Epoch 165/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.4131 - acc: 0.8674 - val_loss: 0.4129 - val_acc: 0.8652\n",
      "Epoch 166/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.4110 - acc: 0.8653 - val_loss: 0.4108 - val_acc: 0.8690\n",
      "Epoch 167/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.4093 - acc: 0.8720 - val_loss: 0.4087 - val_acc: 0.8685\n",
      "Epoch 168/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.4069 - acc: 0.8706 - val_loss: 0.4068 - val_acc: 0.8666\n",
      "Epoch 169/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.4048 - acc: 0.8673 - val_loss: 0.4047 - val_acc: 0.8713\n",
      "Epoch 170/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.4034 - acc: 0.8754 - val_loss: 0.4043 - val_acc: 0.8559\n",
      "Epoch 171/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.4021 - acc: 0.8592 - val_loss: 0.4013 - val_acc: 0.8755\n",
      "Epoch 172/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3992 - acc: 0.8749 - val_loss: 0.4005 - val_acc: 0.8577\n",
      "Epoch 173/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3976 - acc: 0.8618 - val_loss: 0.3983 - val_acc: 0.8769\n",
      "Epoch 174/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3965 - acc: 0.8784 - val_loss: 0.3973 - val_acc: 0.8577\n",
      "Epoch 175/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3941 - acc: 0.8649 - val_loss: 0.3938 - val_acc: 0.8774\n",
      "Epoch 176/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3923 - acc: 0.8733 - val_loss: 0.3917 - val_acc: 0.8690\n",
      "Epoch 177/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3903 - acc: 0.8770 - val_loss: 0.3896 - val_acc: 0.8708\n",
      "Epoch 178/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3883 - acc: 0.8670 - val_loss: 0.3880 - val_acc: 0.8746\n",
      "Epoch 179/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3871 - acc: 0.8808 - val_loss: 0.3861 - val_acc: 0.8708\n",
      "Epoch 180/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3841 - acc: 0.8715 - val_loss: 0.3842 - val_acc: 0.8737\n",
      "Epoch 181/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3823 - acc: 0.8767 - val_loss: 0.3827 - val_acc: 0.8713\n",
      "Epoch 182/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3806 - acc: 0.8733 - val_loss: 0.3808 - val_acc: 0.8723\n",
      "Epoch 183/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3789 - acc: 0.8773 - val_loss: 0.3796 - val_acc: 0.8718\n",
      "Epoch 184/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3775 - acc: 0.8728 - val_loss: 0.3774 - val_acc: 0.8732\n",
      "Epoch 185/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3756 - acc: 0.8759 - val_loss: 0.3763 - val_acc: 0.8723\n",
      "Epoch 186/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3740 - acc: 0.8762 - val_loss: 0.3743 - val_acc: 0.8708\n",
      "Epoch 187/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3722 - acc: 0.8754 - val_loss: 0.3727 - val_acc: 0.8713\n",
      "Epoch 188/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3707 - acc: 0.8779 - val_loss: 0.3710 - val_acc: 0.8727\n",
      "Epoch 189/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3693 - acc: 0.8754 - val_loss: 0.3695 - val_acc: 0.8779\n",
      "Epoch 190/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3680 - acc: 0.8811 - val_loss: 0.3682 - val_acc: 0.8727\n",
      "Epoch 191/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3659 - acc: 0.8782 - val_loss: 0.3665 - val_acc: 0.8727\n",
      "Epoch 192/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3646 - acc: 0.8787 - val_loss: 0.3648 - val_acc: 0.8746\n",
      "Epoch 193/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3629 - acc: 0.8768 - val_loss: 0.3632 - val_acc: 0.8774\n",
      "Epoch 194/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3612 - acc: 0.8810 - val_loss: 0.3619 - val_acc: 0.8737\n",
      "Epoch 195/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3598 - acc: 0.8795 - val_loss: 0.3602 - val_acc: 0.8774\n",
      "Epoch 196/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3583 - acc: 0.8783 - val_loss: 0.3588 - val_acc: 0.8797\n",
      "Epoch 197/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3568 - acc: 0.8824 - val_loss: 0.3577 - val_acc: 0.8751\n",
      "Epoch 198/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3554 - acc: 0.8786 - val_loss: 0.3558 - val_acc: 0.8802\n",
      "Epoch 199/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3539 - acc: 0.8825 - val_loss: 0.3545 - val_acc: 0.8779\n",
      "Epoch 200/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3523 - acc: 0.8796 - val_loss: 0.3530 - val_acc: 0.8816\n",
      "Epoch 201/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3513 - acc: 0.8821 - val_loss: 0.3519 - val_acc: 0.8769\n",
      "Epoch 202/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3498 - acc: 0.8826 - val_loss: 0.3503 - val_acc: 0.8774\n",
      "Epoch 203/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3485 - acc: 0.8791 - val_loss: 0.3489 - val_acc: 0.8830\n",
      "Epoch 204/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3470 - acc: 0.8851 - val_loss: 0.3479 - val_acc: 0.8779\n",
      "Epoch 205/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3457 - acc: 0.8795 - val_loss: 0.3462 - val_acc: 0.8830\n",
      "Epoch 206/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3441 - acc: 0.8826 - val_loss: 0.3451 - val_acc: 0.8788\n",
      "Epoch 207/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3426 - acc: 0.8815 - val_loss: 0.3437 - val_acc: 0.8854\n",
      "Epoch 208/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3416 - acc: 0.8864 - val_loss: 0.3430 - val_acc: 0.8788\n",
      "Epoch 209/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3405 - acc: 0.8809 - val_loss: 0.3411 - val_acc: 0.8863\n",
      "Epoch 210/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3391 - acc: 0.8858 - val_loss: 0.3399 - val_acc: 0.8816\n",
      "Epoch 211/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3376 - acc: 0.8849 - val_loss: 0.3385 - val_acc: 0.8844\n",
      "Epoch 212/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3363 - acc: 0.8838 - val_loss: 0.3372 - val_acc: 0.8868\n",
      "Epoch 213/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3353 - acc: 0.8885 - val_loss: 0.3361 - val_acc: 0.8835\n",
      "Epoch 214/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3338 - acc: 0.8837 - val_loss: 0.3348 - val_acc: 0.8886\n",
      "Epoch 215/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3327 - acc: 0.8892 - val_loss: 0.3339 - val_acc: 0.8825\n",
      "Epoch 216/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3316 - acc: 0.8842 - val_loss: 0.3324 - val_acc: 0.8882\n",
      "Epoch 217/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3302 - acc: 0.8884 - val_loss: 0.3317 - val_acc: 0.8821\n",
      "Epoch 218/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3290 - acc: 0.8855 - val_loss: 0.3303 - val_acc: 0.8891\n",
      "Epoch 219/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3285 - acc: 0.8903 - val_loss: 0.3303 - val_acc: 0.8839\n",
      "Epoch 220/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3276 - acc: 0.8849 - val_loss: 0.3279 - val_acc: 0.8900\n",
      "Epoch 221/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3257 - acc: 0.8883 - val_loss: 0.3267 - val_acc: 0.8886\n",
      "Epoch 222/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3244 - acc: 0.8893 - val_loss: 0.3257 - val_acc: 0.8863\n",
      "Epoch 223/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3232 - acc: 0.8876 - val_loss: 0.3243 - val_acc: 0.8900\n",
      "Epoch 224/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3223 - acc: 0.8912 - val_loss: 0.3244 - val_acc: 0.8835\n",
      "Epoch 225/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3222 - acc: 0.8864 - val_loss: 0.3225 - val_acc: 0.8910\n",
      "Epoch 226/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3204 - acc: 0.8925 - val_loss: 0.3213 - val_acc: 0.8891\n",
      "Epoch 227/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3187 - acc: 0.8904 - val_loss: 0.3199 - val_acc: 0.8910\n",
      "Epoch 228/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3178 - acc: 0.8899 - val_loss: 0.3189 - val_acc: 0.8914\n",
      "Epoch 229/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3166 - acc: 0.8915 - val_loss: 0.3181 - val_acc: 0.8905\n",
      "Epoch 230/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3154 - acc: 0.8893 - val_loss: 0.3167 - val_acc: 0.8924\n",
      "Epoch 231/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3144 - acc: 0.8924 - val_loss: 0.3159 - val_acc: 0.8933\n",
      "Epoch 232/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3133 - acc: 0.8908 - val_loss: 0.3147 - val_acc: 0.8933\n",
      "Epoch 233/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3122 - acc: 0.8920 - val_loss: 0.3141 - val_acc: 0.8928\n",
      "Epoch 234/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3115 - acc: 0.8926 - val_loss: 0.3127 - val_acc: 0.8919\n",
      "Epoch 235/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3104 - acc: 0.8953 - val_loss: 0.3137 - val_acc: 0.8882\n",
      "Epoch 236/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3103 - acc: 0.8892 - val_loss: 0.3110 - val_acc: 0.8933\n",
      "Epoch 237/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3088 - acc: 0.8959 - val_loss: 0.3101 - val_acc: 0.8961\n",
      "Epoch 238/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3076 - acc: 0.8948 - val_loss: 0.3087 - val_acc: 0.8942\n",
      "Epoch 239/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3062 - acc: 0.8931 - val_loss: 0.3077 - val_acc: 0.8952\n",
      "Epoch 240/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3056 - acc: 0.8953 - val_loss: 0.3070 - val_acc: 0.8952\n",
      "Epoch 241/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3042 - acc: 0.8933 - val_loss: 0.3058 - val_acc: 0.8933\n",
      "Epoch 242/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3032 - acc: 0.8949 - val_loss: 0.3049 - val_acc: 0.8942\n",
      "Epoch 243/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3023 - acc: 0.8953 - val_loss: 0.3039 - val_acc: 0.8952\n",
      "Epoch 244/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3013 - acc: 0.8953 - val_loss: 0.3031 - val_acc: 0.8966\n",
      "Epoch 245/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.3008 - acc: 0.8965 - val_loss: 0.3022 - val_acc: 0.8961\n",
      "Epoch 246/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2995 - acc: 0.8958 - val_loss: 0.3012 - val_acc: 0.8966\n",
      "Epoch 247/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2987 - acc: 0.8970 - val_loss: 0.3006 - val_acc: 0.8975\n",
      "Epoch 248/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2976 - acc: 0.8967 - val_loss: 0.2993 - val_acc: 0.8975\n",
      "Epoch 249/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2966 - acc: 0.8980 - val_loss: 0.2995 - val_acc: 0.8989\n",
      "Epoch 250/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2961 - acc: 0.8973 - val_loss: 0.2979 - val_acc: 0.8980\n",
      "Epoch 251/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2951 - acc: 0.8989 - val_loss: 0.2977 - val_acc: 0.8989\n",
      "Epoch 252/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2941 - acc: 0.8981 - val_loss: 0.2957 - val_acc: 0.8999\n",
      "Epoch 253/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2930 - acc: 0.8993 - val_loss: 0.2961 - val_acc: 0.8994\n",
      "Epoch 254/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2930 - acc: 0.8973 - val_loss: 0.2944 - val_acc: 0.8980\n",
      "Epoch 255/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2920 - acc: 0.8993 - val_loss: 0.2931 - val_acc: 0.8994\n",
      "Epoch 256/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2910 - acc: 0.9009 - val_loss: 0.2924 - val_acc: 0.8994\n",
      "Epoch 257/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2897 - acc: 0.8984 - val_loss: 0.2914 - val_acc: 0.9013\n",
      "Epoch 258/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2887 - acc: 0.9007 - val_loss: 0.2906 - val_acc: 0.9008\n",
      "Epoch 259/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2877 - acc: 0.8994 - val_loss: 0.2897 - val_acc: 0.9017\n",
      "Epoch 260/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2872 - acc: 0.9023 - val_loss: 0.2893 - val_acc: 0.9013\n",
      "Epoch 261/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2864 - acc: 0.9001 - val_loss: 0.2880 - val_acc: 0.9022\n",
      "Epoch 262/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2853 - acc: 0.9018 - val_loss: 0.2877 - val_acc: 0.9027\n",
      "Epoch 263/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2842 - acc: 0.9005 - val_loss: 0.2865 - val_acc: 0.9008\n",
      "Epoch 264/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2840 - acc: 0.9031 - val_loss: 0.2861 - val_acc: 0.9050\n",
      "Epoch 265/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2828 - acc: 0.9017 - val_loss: 0.2847 - val_acc: 0.9036\n",
      "Epoch 266/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2819 - acc: 0.9020 - val_loss: 0.2843 - val_acc: 0.9036\n",
      "Epoch 267/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2812 - acc: 0.9014 - val_loss: 0.2833 - val_acc: 0.9022\n",
      "Epoch 268/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2804 - acc: 0.9021 - val_loss: 0.2823 - val_acc: 0.9041\n",
      "Epoch 269/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2794 - acc: 0.9030 - val_loss: 0.2818 - val_acc: 0.9031\n",
      "Epoch 270/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2787 - acc: 0.9028 - val_loss: 0.2809 - val_acc: 0.9027\n",
      "Epoch 271/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2781 - acc: 0.9028 - val_loss: 0.2805 - val_acc: 0.9050\n",
      "Epoch 272/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2781 - acc: 0.9055 - val_loss: 0.2820 - val_acc: 0.8989\n",
      "Epoch 273/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2782 - acc: 0.9005 - val_loss: 0.2798 - val_acc: 0.9059\n",
      "Epoch 274/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2768 - acc: 0.9045 - val_loss: 0.2787 - val_acc: 0.9059\n",
      "Epoch 275/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2750 - acc: 0.9031 - val_loss: 0.2768 - val_acc: 0.9059\n",
      "Epoch 276/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2740 - acc: 0.9058 - val_loss: 0.2764 - val_acc: 0.9055\n",
      "Epoch 277/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2733 - acc: 0.9048 - val_loss: 0.2754 - val_acc: 0.9045\n",
      "Epoch 278/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2724 - acc: 0.9051 - val_loss: 0.2746 - val_acc: 0.9055\n",
      "Epoch 279/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2721 - acc: 0.9068 - val_loss: 0.2755 - val_acc: 0.9055\n",
      "Epoch 280/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2723 - acc: 0.9043 - val_loss: 0.2739 - val_acc: 0.9088\n",
      "Epoch 281/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2713 - acc: 0.9087 - val_loss: 0.2745 - val_acc: 0.9050\n",
      "Epoch 282/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2707 - acc: 0.9020 - val_loss: 0.2741 - val_acc: 0.9059\n",
      "Epoch 283/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2723 - acc: 0.9089 - val_loss: 0.2764 - val_acc: 0.8975\n",
      "Epoch 284/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2707 - acc: 0.9015 - val_loss: 0.2724 - val_acc: 0.9069\n",
      "Epoch 285/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2686 - acc: 0.9099 - val_loss: 0.2737 - val_acc: 0.8989\n",
      "Epoch 286/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2685 - acc: 0.9045 - val_loss: 0.2704 - val_acc: 0.9092\n",
      "Epoch 287/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2670 - acc: 0.9098 - val_loss: 0.2700 - val_acc: 0.9069\n",
      "Epoch 288/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2653 - acc: 0.9083 - val_loss: 0.2683 - val_acc: 0.9097\n",
      "Epoch 289/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2651 - acc: 0.9112 - val_loss: 0.2678 - val_acc: 0.9069\n",
      "Epoch 290/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2639 - acc: 0.9076 - val_loss: 0.2664 - val_acc: 0.9120\n",
      "Epoch 291/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2631 - acc: 0.9110 - val_loss: 0.2667 - val_acc: 0.9078\n",
      "Epoch 292/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2628 - acc: 0.9077 - val_loss: 0.2650 - val_acc: 0.9120\n",
      "Epoch 293/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2623 - acc: 0.9098 - val_loss: 0.2645 - val_acc: 0.9083\n",
      "Epoch 294/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2609 - acc: 0.9101 - val_loss: 0.2633 - val_acc: 0.9111\n",
      "Epoch 295/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2602 - acc: 0.9115 - val_loss: 0.2633 - val_acc: 0.9083\n",
      "Epoch 296/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2597 - acc: 0.9099 - val_loss: 0.2620 - val_acc: 0.9134\n",
      "Epoch 297/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2589 - acc: 0.9111 - val_loss: 0.2614 - val_acc: 0.9111\n",
      "Epoch 298/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2584 - acc: 0.9130 - val_loss: 0.2608 - val_acc: 0.9111\n",
      "Epoch 299/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2575 - acc: 0.9110 - val_loss: 0.2601 - val_acc: 0.9130\n",
      "Epoch 300/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2572 - acc: 0.9132 - val_loss: 0.2600 - val_acc: 0.9083\n",
      "Epoch 301/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2565 - acc: 0.9106 - val_loss: 0.2586 - val_acc: 0.9125\n",
      "Epoch 302/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2557 - acc: 0.9128 - val_loss: 0.2580 - val_acc: 0.9130\n",
      "Epoch 303/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2549 - acc: 0.9138 - val_loss: 0.2575 - val_acc: 0.9116\n",
      "Epoch 304/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2541 - acc: 0.9134 - val_loss: 0.2568 - val_acc: 0.9130\n",
      "Epoch 305/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2535 - acc: 0.9134 - val_loss: 0.2560 - val_acc: 0.9139\n",
      "Epoch 306/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2528 - acc: 0.9141 - val_loss: 0.2558 - val_acc: 0.9116\n",
      "Epoch 307/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2523 - acc: 0.9135 - val_loss: 0.2548 - val_acc: 0.9134\n",
      "Epoch 308/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2515 - acc: 0.9149 - val_loss: 0.2544 - val_acc: 0.9120\n",
      "Epoch 309/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2508 - acc: 0.9141 - val_loss: 0.2535 - val_acc: 0.9148\n",
      "Epoch 310/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2504 - acc: 0.9146 - val_loss: 0.2529 - val_acc: 0.9153\n",
      "Epoch 311/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2500 - acc: 0.9161 - val_loss: 0.2527 - val_acc: 0.9116\n",
      "Epoch 312/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2494 - acc: 0.9140 - val_loss: 0.2516 - val_acc: 0.9144\n",
      "Epoch 313/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2484 - acc: 0.9156 - val_loss: 0.2511 - val_acc: 0.9144\n",
      "Epoch 314/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2478 - acc: 0.9166 - val_loss: 0.2510 - val_acc: 0.9125\n",
      "Epoch 315/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2478 - acc: 0.9128 - val_loss: 0.2504 - val_acc: 0.9176\n",
      "Epoch 316/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2485 - acc: 0.9152 - val_loss: 0.2517 - val_acc: 0.9102\n",
      "Epoch 317/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2484 - acc: 0.9108 - val_loss: 0.2489 - val_acc: 0.9167\n",
      "Epoch 318/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2458 - acc: 0.9187 - val_loss: 0.2494 - val_acc: 0.9116\n",
      "Epoch 319/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2452 - acc: 0.9127 - val_loss: 0.2478 - val_acc: 0.9172\n",
      "Epoch 320/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2446 - acc: 0.9177 - val_loss: 0.2477 - val_acc: 0.9139\n",
      "Epoch 321/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2438 - acc: 0.9153 - val_loss: 0.2465 - val_acc: 0.9172\n",
      "Epoch 322/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2436 - acc: 0.9182 - val_loss: 0.2481 - val_acc: 0.9102\n",
      "Epoch 323/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2441 - acc: 0.9137 - val_loss: 0.2451 - val_acc: 0.9167\n",
      "Epoch 324/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2417 - acc: 0.9186 - val_loss: 0.2449 - val_acc: 0.9158\n",
      "Epoch 325/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2411 - acc: 0.9193 - val_loss: 0.2439 - val_acc: 0.9162\n",
      "Epoch 326/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2410 - acc: 0.9163 - val_loss: 0.2432 - val_acc: 0.9162\n",
      "Epoch 327/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2399 - acc: 0.9201 - val_loss: 0.2428 - val_acc: 0.9162\n",
      "Epoch 328/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2392 - acc: 0.9177 - val_loss: 0.2421 - val_acc: 0.9167\n",
      "Epoch 329/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2388 - acc: 0.9200 - val_loss: 0.2419 - val_acc: 0.9167\n",
      "Epoch 330/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2381 - acc: 0.9192 - val_loss: 0.2410 - val_acc: 0.9176\n",
      "Epoch 331/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2376 - acc: 0.9206 - val_loss: 0.2407 - val_acc: 0.9167\n",
      "Epoch 332/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2369 - acc: 0.9204 - val_loss: 0.2398 - val_acc: 0.9167\n",
      "Epoch 333/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2364 - acc: 0.9215 - val_loss: 0.2397 - val_acc: 0.9172\n",
      "Epoch 334/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2361 - acc: 0.9196 - val_loss: 0.2392 - val_acc: 0.9176\n",
      "Epoch 335/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2357 - acc: 0.9187 - val_loss: 0.2383 - val_acc: 0.9209\n",
      "Epoch 336/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2354 - acc: 0.9207 - val_loss: 0.2381 - val_acc: 0.9186\n",
      "Epoch 337/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2344 - acc: 0.9200 - val_loss: 0.2371 - val_acc: 0.9209\n",
      "Epoch 338/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2342 - acc: 0.9215 - val_loss: 0.2368 - val_acc: 0.9186\n",
      "Epoch 339/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2328 - acc: 0.9208 - val_loss: 0.2360 - val_acc: 0.9219\n",
      "Epoch 340/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2325 - acc: 0.9224 - val_loss: 0.2360 - val_acc: 0.9181\n",
      "Epoch 341/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2318 - acc: 0.9224 - val_loss: 0.2348 - val_acc: 0.9200\n",
      "Epoch 342/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2315 - acc: 0.9215 - val_loss: 0.2343 - val_acc: 0.9209\n",
      "Epoch 343/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2324 - acc: 0.9231 - val_loss: 0.2360 - val_acc: 0.9162\n",
      "Epoch 344/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2320 - acc: 0.9193 - val_loss: 0.2336 - val_acc: 0.9228\n",
      "Epoch 345/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2304 - acc: 0.9231 - val_loss: 0.2346 - val_acc: 0.9167\n",
      "Epoch 346/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2298 - acc: 0.9208 - val_loss: 0.2322 - val_acc: 0.9228\n",
      "Epoch 347/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2287 - acc: 0.9230 - val_loss: 0.2326 - val_acc: 0.9172\n",
      "Epoch 348/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2283 - acc: 0.9225 - val_loss: 0.2313 - val_acc: 0.9233\n",
      "Epoch 349/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2287 - acc: 0.9234 - val_loss: 0.2322 - val_acc: 0.9167\n",
      "Epoch 350/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2277 - acc: 0.9202 - val_loss: 0.2305 - val_acc: 0.9237\n",
      "Epoch 351/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2270 - acc: 0.9247 - val_loss: 0.2315 - val_acc: 0.9167\n",
      "Epoch 352/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2267 - acc: 0.9215 - val_loss: 0.2289 - val_acc: 0.9242\n",
      "Epoch 353/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2253 - acc: 0.9247 - val_loss: 0.2293 - val_acc: 0.9181\n",
      "Epoch 354/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2247 - acc: 0.9248 - val_loss: 0.2281 - val_acc: 0.9233\n",
      "Epoch 355/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2249 - acc: 0.9250 - val_loss: 0.2285 - val_acc: 0.9181\n",
      "Epoch 356/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2242 - acc: 0.9232 - val_loss: 0.2272 - val_acc: 0.9247\n",
      "Epoch 357/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2236 - acc: 0.9251 - val_loss: 0.2279 - val_acc: 0.9186\n",
      "Epoch 358/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2231 - acc: 0.9236 - val_loss: 0.2259 - val_acc: 0.9251\n",
      "Epoch 359/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2228 - acc: 0.9258 - val_loss: 0.2260 - val_acc: 0.9204\n",
      "Epoch 360/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2216 - acc: 0.9268 - val_loss: 0.2248 - val_acc: 0.9265\n",
      "Epoch 361/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2213 - acc: 0.9259 - val_loss: 0.2250 - val_acc: 0.9204\n",
      "Epoch 362/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2208 - acc: 0.9256 - val_loss: 0.2238 - val_acc: 0.9270\n",
      "Epoch 363/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2200 - acc: 0.9263 - val_loss: 0.2242 - val_acc: 0.9204\n",
      "Epoch 364/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2196 - acc: 0.9254 - val_loss: 0.2242 - val_acc: 0.9256\n",
      "Epoch 365/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2213 - acc: 0.9251 - val_loss: 0.2252 - val_acc: 0.9186\n",
      "Epoch 366/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2202 - acc: 0.9235 - val_loss: 0.2219 - val_acc: 0.9261\n",
      "Epoch 367/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2188 - acc: 0.9277 - val_loss: 0.2232 - val_acc: 0.9204\n",
      "Epoch 368/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2187 - acc: 0.9249 - val_loss: 0.2210 - val_acc: 0.9275\n",
      "Epoch 369/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2174 - acc: 0.9272 - val_loss: 0.2211 - val_acc: 0.9233\n",
      "Epoch 370/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2167 - acc: 0.9266 - val_loss: 0.2198 - val_acc: 0.9265\n",
      "Epoch 371/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2159 - acc: 0.9282 - val_loss: 0.2194 - val_acc: 0.9270\n",
      "Epoch 372/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2155 - acc: 0.9282 - val_loss: 0.2188 - val_acc: 0.9275\n",
      "Epoch 373/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2151 - acc: 0.9289 - val_loss: 0.2187 - val_acc: 0.9270\n",
      "Epoch 374/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2146 - acc: 0.9279 - val_loss: 0.2179 - val_acc: 0.9275\n",
      "Epoch 375/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2139 - acc: 0.9292 - val_loss: 0.2178 - val_acc: 0.9265\n",
      "Epoch 376/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2134 - acc: 0.9285 - val_loss: 0.2169 - val_acc: 0.9284\n",
      "Epoch 377/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2130 - acc: 0.9292 - val_loss: 0.2165 - val_acc: 0.9284\n",
      "Epoch 378/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2126 - acc: 0.9298 - val_loss: 0.2163 - val_acc: 0.9275\n",
      "Epoch 379/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2122 - acc: 0.9282 - val_loss: 0.2155 - val_acc: 0.9284\n",
      "Epoch 380/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2116 - acc: 0.9297 - val_loss: 0.2157 - val_acc: 0.9261\n",
      "Epoch 381/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2115 - acc: 0.9280 - val_loss: 0.2146 - val_acc: 0.9312\n",
      "Epoch 382/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2108 - acc: 0.9300 - val_loss: 0.2149 - val_acc: 0.9251\n",
      "Epoch 383/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2107 - acc: 0.9290 - val_loss: 0.2137 - val_acc: 0.9312\n",
      "Epoch 384/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2103 - acc: 0.9300 - val_loss: 0.2142 - val_acc: 0.9247\n",
      "Epoch 385/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2094 - acc: 0.9287 - val_loss: 0.2129 - val_acc: 0.9312\n",
      "Epoch 386/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2092 - acc: 0.9303 - val_loss: 0.2129 - val_acc: 0.9261\n",
      "Epoch 387/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2086 - acc: 0.9307 - val_loss: 0.2119 - val_acc: 0.9312\n",
      "Epoch 388/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2077 - acc: 0.9304 - val_loss: 0.2113 - val_acc: 0.9307\n",
      "Epoch 389/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2072 - acc: 0.9313 - val_loss: 0.2112 - val_acc: 0.9284\n",
      "Epoch 390/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2069 - acc: 0.9311 - val_loss: 0.2106 - val_acc: 0.9317\n",
      "Epoch 391/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2073 - acc: 0.9309 - val_loss: 0.2108 - val_acc: 0.9261\n",
      "Epoch 392/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2070 - acc: 0.9302 - val_loss: 0.2096 - val_acc: 0.9312\n",
      "Epoch 393/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2061 - acc: 0.9318 - val_loss: 0.2099 - val_acc: 0.9261\n",
      "Epoch 394/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2051 - acc: 0.9312 - val_loss: 0.2085 - val_acc: 0.9321\n",
      "Epoch 395/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2042 - acc: 0.9321 - val_loss: 0.2088 - val_acc: 0.9284\n",
      "Epoch 396/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2041 - acc: 0.9321 - val_loss: 0.2079 - val_acc: 0.9326\n",
      "Epoch 397/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2040 - acc: 0.9330 - val_loss: 0.2080 - val_acc: 0.9279\n",
      "Epoch 398/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2034 - acc: 0.9321 - val_loss: 0.2068 - val_acc: 0.9331\n",
      "Epoch 399/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2025 - acc: 0.9321 - val_loss: 0.2063 - val_acc: 0.9321\n",
      "Epoch 400/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2021 - acc: 0.9328 - val_loss: 0.2058 - val_acc: 0.9331\n",
      "Epoch 401/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2016 - acc: 0.9327 - val_loss: 0.2055 - val_acc: 0.9326\n",
      "Epoch 402/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2011 - acc: 0.9327 - val_loss: 0.2055 - val_acc: 0.9303\n",
      "Epoch 403/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2010 - acc: 0.9333 - val_loss: 0.2045 - val_acc: 0.9340\n",
      "Epoch 404/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.2005 - acc: 0.9328 - val_loss: 0.2043 - val_acc: 0.9326\n",
      "Epoch 405/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.1998 - acc: 0.9328 - val_loss: 0.2037 - val_acc: 0.9331\n",
      "Epoch 406/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.1993 - acc: 0.9328 - val_loss: 0.2032 - val_acc: 0.9331\n",
      "Epoch 407/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.1988 - acc: 0.9335 - val_loss: 0.2028 - val_acc: 0.9326\n",
      "Epoch 408/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.1987 - acc: 0.9331 - val_loss: 0.2033 - val_acc: 0.9289\n",
      "Epoch 409/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.1987 - acc: 0.9345 - val_loss: 0.2024 - val_acc: 0.9350\n",
      "Epoch 410/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.1988 - acc: 0.9332 - val_loss: 0.2031 - val_acc: 0.9270\n",
      "Epoch 411/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.1976 - acc: 0.9352 - val_loss: 0.2013 - val_acc: 0.9354\n",
      "Epoch 412/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.1970 - acc: 0.9337 - val_loss: 0.2016 - val_acc: 0.9298\n",
      "Epoch 413/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.1964 - acc: 0.9347 - val_loss: 0.2002 - val_acc: 0.9340\n",
      "Epoch 414/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.1957 - acc: 0.9347 - val_loss: 0.2003 - val_acc: 0.9321\n",
      "Epoch 415/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.1954 - acc: 0.9358 - val_loss: 0.1993 - val_acc: 0.9336\n",
      "Epoch 416/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.1949 - acc: 0.9344 - val_loss: 0.2002 - val_acc: 0.9303\n",
      "Epoch 417/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.1955 - acc: 0.9352 - val_loss: 0.1992 - val_acc: 0.9350\n",
      "Epoch 418/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.1962 - acc: 0.9358 - val_loss: 0.1997 - val_acc: 0.9289\n",
      "Epoch 419/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.1943 - acc: 0.9368 - val_loss: 0.1979 - val_acc: 0.9364\n",
      "Epoch 420/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.1939 - acc: 0.9353 - val_loss: 0.1984 - val_acc: 0.9307\n",
      "Epoch 421/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.1932 - acc: 0.9368 - val_loss: 0.1974 - val_acc: 0.9359\n",
      "Epoch 422/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.1937 - acc: 0.9364 - val_loss: 0.1976 - val_acc: 0.9307\n",
      "Epoch 423/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.1923 - acc: 0.9376 - val_loss: 0.1960 - val_acc: 0.9364\n",
      "Epoch 424/500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7cc6590e1a90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m history = model.fit(X_train500, Y_train,\n\u001b[1;32m     31\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                     verbose=1, validation_data=(X_test500, Y_test))\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/xupeng.tong/anaconda/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    407\u001b[0m                               \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m                               sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/Users/xupeng.tong/anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight)\u001b[0m\n\u001b[1;32m   1050\u001b[0m                               \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m                               callback_metrics=callback_metrics)\n\u001b[0m\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/xupeng.tong/anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics)\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/xupeng.tong/anaconda/lib/python2.7/site-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/xupeng.tong/anaconda/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/xupeng.tong/anaconda/lib/python2.7/site-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n)\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m                     \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/xupeng.tong/anaconda/lib/python2.7/site-packages/theano/tensor/blas.pyc\u001b[0m in \u001b[0;36mperform\u001b[0;34m(self, node, inp, out)\u001b[0m\n\u001b[1;32m   1821\u001b[0m         \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m             \u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscalar\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m             \u001b[0;31m# The error raised by numpy has no shape information, we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.utils import np_utils\n",
    "\n",
    "batch_size = 2000\n",
    "nb_classes = 2\n",
    "nb_epoch = 500\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(300, input_shape=(500,)))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train500, Y_train,\n",
    "                    batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "                    verbose=1, validation_data=(X_test500, Y_test))\n",
    "score = model.evaluate(X_test500, Y_test, verbose=0)\n",
    "\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train_tf = y_train.reshape(y_train.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Run id: 1WULU2\n",
      "Log directory: /tmp/tflearn_logs/\n",
      "---------------------------------\n",
      "Training samples: 8547\n",
      "Validation samples: 2137\n",
      "--\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (64, 500) for Tensor u'InputData/X:0', which has shape '(?, 7203)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-d8050724e459>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtflearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/xupeng.tong/anaconda/lib/python2.7/site-packages/tflearn/models/dnn.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_inputs, Y_targets, n_epoch, validation_set, show_metric, batch_size, shuffle, snapshot_epoch, snapshot_step, excl_trainops, run_id)\u001b[0m\n\u001b[1;32m    186\u001b[0m                          \u001b[0mdaug_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdaug_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                          \u001b[0mexcl_trainops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexcl_trainops\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m                          run_id=run_id)\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/xupeng.tong/anaconda/lib/python2.7/site-packages/tflearn/helpers/trainer.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, feed_dicts, n_epoch, val_feed_dicts, show_metric, snapshot_step, snapshot_epoch, shuffle_all, dprep_dict, daug_dict, excl_trainops, run_id)\u001b[0m\n\u001b[1;32m    275\u001b[0m                                                        \u001b[0msnapshot_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                                                        \u001b[0msnapshot_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                                                        show_metric)\n\u001b[0m\u001b[1;32m    278\u001b[0m                             \u001b[0mglobal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macc_value\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mglobal_acc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/xupeng.tong/anaconda/lib/python2.7/site-packages/tflearn/helpers/trainer.pyc\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, training_step, snapshot_epoch, snapshot_step, show_metric)\u001b[0m\n\u001b[1;32m    682\u001b[0m         \u001b[0mtflearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m         _, train_summ_str = self.session.run([self.train, self.summ_op],\n\u001b[0;32m--> 684\u001b[0;31m                                              feed_batch)\n\u001b[0m\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0;31m# Retrieve loss value from summary string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/xupeng.tong/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 340\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    341\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/xupeng.tong/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m                 \u001b[0;34m'which has shape %r'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m    554\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (64, 500) for Tensor u'InputData/X:0', which has shape '(?, 7203)'"
     ]
    }
   ],
   "source": [
    "import tflearn\n",
    "import tensorflow as tf\n",
    "\n",
    "# Classification\n",
    "# tflearn.init_graph(num_cores=8, gpu_memory_fraction=0.5)\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    inputlayer = tflearn.input_data(shape=[None, 7203])\n",
    "    dense = tflearn.fully_connected(inputlayer, 300, activation='tanh')\n",
    "    sofmax = tflearn.fully_connected(dense, 2, activation='softmax')\n",
    "    net = tflearn.regression(sofmax, optimizer='sgd', loss='categorical_crossentropy')\n",
    "\n",
    "    model = tflearn.DNN(net)\n",
    "    model.fit(X_train500, Y_train, n_epoch=10, show_metric=True, validation_set=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "temp = abs(dfsMLP.selected_ws[0])\n",
    "\n",
    "plt.plot(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp = dfsMLP.selected_ws\n",
    "\n",
    "temp1, temp2 = temp[0], temp[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.where(abs(temp1)<0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.where(abs(temp2)<0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp = dfsMLP.selected_ws\n",
    "\n",
    "temp3, temp4 = temp[0], temp[1]\n",
    "\n",
    "print(np.where(abs(temp3)<0.01))\n",
    "print(np.where(abs(temp4)<0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp = dfsMLP.selected_ws\n",
    "\n",
    "temp5, temp6 = temp[0], temp[1]\n",
    "\n",
    "print(np.where(abs(temp5)<0.01))\n",
    "print(np.where(abs(temp6)<0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# rf = RandomForestClassifier(criterion=\"entropy\", n_estimators = 300, max_depth = 100)\n",
    "# rf.fit(X_train, y_train)\n",
    "\n",
    "# y_pred = rf.predict(X_test)\n",
    "\n",
    "gbm = xgb.XGBClassifier(max_depth=3, n_estimators=400, learning_rate=0.05).fit(X_train, y_train)\n",
    "y_pred = gbm.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "# gbm10 = xgb.XGBClassifier(max_depth=3, n_estimators=400, learning_rate=0.05).fit(X_train10, y_train)\n",
    "# y_pred10 = gbm.predict(X_test10)\n",
    "\n",
    "# print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indexes = np.argsort(gbm.feature_importances_)[::-1]\n",
    "\n",
    "# top100Features = np.array([columnNames[0][i] for i in indexes[0:200]])\n",
    "# top100Features = np.array([columnNames[i] for i in indexes[0:100]])\n",
    "# top100Features = top100Features.reshape(100,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train5, X_test5 = X_train[:, indexes[:5]], X_test[:, indexes[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "clf = SGDClassifier(loss=\"log\", penalty=\"l1\")\n",
    "\n",
    "clf.fit(X_train500, y_train)\n",
    "\n",
    "y_pred500lr = clf.predict(X_test500)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred500lr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
