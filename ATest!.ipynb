{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from scipy import io as sio\n",
    "from tensorflow.python.framework import ops\n",
    "from dfs2 import DeepFeatureSelectionNew\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "ourdataB = sio.loadmat(\"/Volumes/TONY/Regeneron/Data/OriginalData/newDataB_2labels.mat\")\n",
    "inputX = ourdataB['X']\n",
    "inputX = normalize(inputX, axis=0)\n",
    "inputY = ourdataB['Y'][0,:]\n",
    "columnNames = ourdataB['columnNames']\n",
    "\n",
    "# iris = datasets.load_iris()\n",
    "# inputX = iris.data[:,[1,2,3]]\n",
    "# inputY = iris.target\n",
    "\n",
    "# digits = datasets.load_digits()\n",
    "# inputX = digits.data  \n",
    "# inputY = digits.target\n",
    "\n",
    "# inputX, inputY = make_classification(n_samples=1000, n_features=7500, n_informative=3000, n_redundant=0, n_repeated=0, n_classes=2)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputX, inputY, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indexes = sio.loadmat(\"xgboost_result\")['importance_rank']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train500, X_test500 = X_train[:, indexes.tolist()[0][:500]], X_test[:, indexes.tolist()[0][:500]]\n",
    "    \n",
    "X_train10, X_test10 = X_train[:, indexes.tolist()[0][:10]], X_test[:, indexes.tolist()[0][:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(indexes.tolist()[0][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: global loss = 14.4005060196\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 5: global loss = 1.21139788628\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 10: global loss = 0.974212765694\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 15: global loss = 0.951200664043\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 20: global loss = 0.934549212456\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 25: global loss = 0.91210103035\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 30: global loss = 0.895365476608\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 35: global loss = 0.877996623516\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 40: global loss = 0.864667713642\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 45: global loss = 0.84937864542\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 50: global loss = 0.837174296379\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 55: global loss = 0.82801502943\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 60: global loss = 0.820423185825\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 65: global loss = 0.812409818172\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 70: global loss = 0.806577265263\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 75: global loss = 0.801768362522\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 80: global loss = 0.800285339355\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 85: global loss = 0.79391503334\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 90: global loss = 0.793439328671\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 95: global loss = 0.79012554884\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 100: global loss = 0.792232275009\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 105: global loss = 0.78844755888\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 110: global loss = 0.789217352867\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 115: global loss = 0.78856754303\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 120: global loss = 0.790014564991\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 125: global loss = 0.788425981998\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 130: global loss = 0.78829485178\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 135: global loss = 0.789459228516\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 140: global loss = 0.791053652763\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 145: global loss = 0.788894891739\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 150: global loss = 0.789866030216\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 155: global loss = 0.788646221161\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 160: global loss = 0.791886150837\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 165: global loss = 0.78805321455\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 170: global loss = 0.790146172047\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 175: global loss = 0.789125144482\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 180: global loss = 0.791800379753\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 185: global loss = 0.789343714714\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 190: global loss = 0.789227366447\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 195: global loss = 0.789785683155\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 200: global loss = 0.790615439415\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 205: global loss = 0.78921020031\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 210: global loss = 0.789525568485\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 215: global loss = 0.78913462162\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 220: global loss = 0.791587889194\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 225: global loss = 0.788051843643\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 230: global loss = 0.789773643017\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 235: global loss = 0.788202106953\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 240: global loss = 0.791458368301\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 245: global loss = 0.788226246834\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 250: global loss = 0.789357721806\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 255: global loss = 0.789102315903\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 260: global loss = 0.790466964245\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 265: global loss = 0.788721382618\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 270: global loss = 0.788245856762\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 275: global loss = 0.789141774178\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 280: global loss = 0.790691673756\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 285: global loss = 0.788251638412\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 290: global loss = 0.789272427559\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 295: global loss = 0.787929415703\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 300: global loss = 0.791290700436\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 305: global loss = 0.78717982769\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 310: global loss = 0.789215087891\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 315: global loss = 0.788186013699\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 320: global loss = 0.790644407272\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 325: global loss = 0.788165926933\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 330: global loss = 0.788077294827\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 335: global loss = 0.788738131523\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 340: global loss = 0.789562404156\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 345: global loss = 0.788112640381\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 350: global loss = 0.7884760499\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 355: global loss = 0.788151860237\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 360: global loss = 0.790565907955\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 365: global loss = 0.78700876236\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 370: global loss = 0.788815200329\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 375: global loss = 0.787126481533\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 380: global loss = 0.790450572968\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 385: global loss = 0.787292063236\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 390: global loss = 0.788370549679\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 395: global loss = 0.788054704666\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 400: global loss = 0.789464592934\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 405: global loss = 0.78766977787\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 410: global loss = 0.787161946297\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n",
      "epoch 415: global loss = 0.788036823273\n",
      "('Train accuracy:', 0.57131159)\n",
      "('Test accuracy:', 0.56621432)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-8be1bb3ee72f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdfsMLP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDeepFeatureSelectionNew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m                                  \u001b[0mlambda1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m,\u001b[0m                                  \u001b[0mweight_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mlp'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdfsMLP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Volumes/TONY/Regeneron/Codes/DeepFeatureSelection--Tensorflow/dfs2.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_batch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                 \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_X\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_Y\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mn_batch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                     \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_X\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_Y\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/TONY/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munique_fetch_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[0;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/TONY/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, target_list, fetch_list, feed_dict)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m       return tf_session.TF_Run(self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 404\u001b[0;31m                                target_list)\n\u001b[0m\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatusNotOK\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ops.reset_default_graph()\n",
    "\n",
    "dfsMLP = DeepFeatureSelectionNew(X_train500, X_test500, y_train, y_test, n_input=0, hidden_dims=[300], learning_rate=0.01, \\\n",
    "                                 lambda1=0.001, lambda2=0.1, alpha1=0.01, alpha2=0.05, activation='sigmoid', \\\n",
    "                                 weight_init='mlp',epochs=100000, optimizer='Adam', print_step=5)\n",
    "\n",
    "dfsMLP.train(batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_3 (Dense)                  (None, 300)           150300      dense_input_2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 300)           0           dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 2)             602         activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 2)             0           dense_4[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 150902\n",
      "____________________________________________________________________________________________________\n",
      "Epoch 1/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.7727 - acc: 0.4363     \n",
      "Epoch 2/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.7095 - acc: 0.5713     \n",
      "Epoch 3/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.7032 - acc: 0.5713     \n",
      "Epoch 4/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6867 - acc: 0.5670     \n",
      "Epoch 5/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6926 - acc: 0.4987     \n",
      "Epoch 6/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6837 - acc: 0.5713     \n",
      "Epoch 7/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6867 - acc: 0.5713     \n",
      "Epoch 8/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6814 - acc: 0.5713     \n",
      "Epoch 9/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6823 - acc: 0.5713     \n",
      "Epoch 10/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6805 - acc: 0.5713     \n",
      "Epoch 11/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6804 - acc: 0.5713     \n",
      "Epoch 12/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6791 - acc: 0.5713     \n",
      "Epoch 13/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6788 - acc: 0.5713     \n",
      "Epoch 14/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6782 - acc: 0.5713     \n",
      "Epoch 15/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6777 - acc: 0.5713     \n",
      "Epoch 16/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6769 - acc: 0.5713     \n",
      "Epoch 17/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6764 - acc: 0.5713     \n",
      "Epoch 18/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6762 - acc: 0.5713     \n",
      "Epoch 19/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6755 - acc: 0.5713     \n",
      "Epoch 20/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6748 - acc: 0.5713     \n",
      "Epoch 21/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6745 - acc: 0.5713     \n",
      "Epoch 22/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6735 - acc: 0.5713     \n",
      "Epoch 23/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6730 - acc: 0.5713     \n",
      "Epoch 24/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6725 - acc: 0.5713     \n",
      "Epoch 25/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6714 - acc: 0.5713     \n",
      "Epoch 26/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6712 - acc: 0.5713     \n",
      "Epoch 27/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6702 - acc: 0.5713     \n",
      "Epoch 28/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6697 - acc: 0.5713     \n",
      "Epoch 29/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6690 - acc: 0.5713     \n",
      "Epoch 30/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6682 - acc: 0.5713     \n",
      "Epoch 31/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6674 - acc: 0.5713     \n",
      "Epoch 32/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6668 - acc: 0.5713     \n",
      "Epoch 33/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6661 - acc: 0.5713     \n",
      "Epoch 34/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6651 - acc: 0.5713     \n",
      "Epoch 35/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6650 - acc: 0.5713     \n",
      "Epoch 36/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6641 - acc: 0.5713     \n",
      "Epoch 37/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6629 - acc: 0.5713     \n",
      "Epoch 38/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6632 - acc: 0.5713     \n",
      "Epoch 39/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6616 - acc: 0.5713     \n",
      "Epoch 40/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6610 - acc: 0.5713     \n",
      "Epoch 41/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6600 - acc: 0.5713     \n",
      "Epoch 42/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6589 - acc: 0.5713     \n",
      "Epoch 43/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6584 - acc: 0.5713     \n",
      "Epoch 44/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6574 - acc: 0.5713     \n",
      "Epoch 45/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6565 - acc: 0.5713     \n",
      "Epoch 46/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6555 - acc: 0.5714     \n",
      "Epoch 47/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6550 - acc: 0.5713     \n",
      "Epoch 48/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6537 - acc: 0.5713     \n",
      "Epoch 49/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6530 - acc: 0.5714     \n",
      "Epoch 50/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6525 - acc: 0.5713     \n",
      "Epoch 51/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6509 - acc: 0.5713     \n",
      "Epoch 52/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6501 - acc: 0.5714     \n",
      "Epoch 53/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6491 - acc: 0.5715     \n",
      "Epoch 54/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6482 - acc: 0.5714     \n",
      "Epoch 55/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6473 - acc: 0.5770     \n",
      "Epoch 56/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6461 - acc: 0.5722     \n",
      "Epoch 57/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6453 - acc: 0.5714     \n",
      "Epoch 58/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6450 - acc: 0.6237     \n",
      "Epoch 59/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6429 - acc: 0.5754     \n",
      "Epoch 60/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6421 - acc: 0.5718     \n",
      "Epoch 61/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6407 - acc: 0.5838     \n",
      "Epoch 62/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6395 - acc: 0.5747     \n",
      "Epoch 63/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6385 - acc: 0.5939     \n",
      "Epoch 64/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6374 - acc: 0.5801     \n",
      "Epoch 65/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6360 - acc: 0.5918     \n",
      "Epoch 66/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6347 - acc: 0.5875     \n",
      "Epoch 67/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6337 - acc: 0.6130     \n",
      "Epoch 68/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6325 - acc: 0.5772     \n",
      "Epoch 69/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6310 - acc: 0.6189     \n",
      "Epoch 70/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6297 - acc: 0.6330     \n",
      "Epoch 71/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6282 - acc: 0.5894     \n",
      "Epoch 72/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6267 - acc: 0.6283     \n",
      "Epoch 73/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6259 - acc: 0.5968     \n",
      "Epoch 74/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6239 - acc: 0.6132     \n",
      "Epoch 75/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6224 - acc: 0.6299     \n",
      "Epoch 76/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6210 - acc: 0.6481     \n",
      "Epoch 77/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6202 - acc: 0.5945     \n",
      "Epoch 78/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6178 - acc: 0.6694     \n",
      "Epoch 79/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6164 - acc: 0.6293     \n",
      "Epoch 80/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6151 - acc: 0.6753     \n",
      "Epoch 81/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6131 - acc: 0.6650     \n",
      "Epoch 82/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6114 - acc: 0.6392     \n",
      "Epoch 83/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6099 - acc: 0.7139     \n",
      "Epoch 84/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6078 - acc: 0.6738     \n",
      "Epoch 85/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6063 - acc: 0.6511     \n",
      "Epoch 86/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6043 - acc: 0.7016     \n",
      "Epoch 87/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6026 - acc: 0.7011     \n",
      "Epoch 88/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.6006 - acc: 0.7042     \n",
      "Epoch 89/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.5988 - acc: 0.6953     \n",
      "Epoch 90/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.5968 - acc: 0.6944     \n",
      "Epoch 91/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.5953 - acc: 0.7624     \n",
      "Epoch 92/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.5932 - acc: 0.6808     \n",
      "Epoch 93/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.5919 - acc: 0.8016     \n",
      "Epoch 94/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.5901 - acc: 0.6660     \n",
      "Epoch 95/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.5871 - acc: 0.7771     \n",
      "Epoch 96/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.5856 - acc: 0.6957     \n",
      "Epoch 97/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.5826 - acc: 0.7786     \n",
      "Epoch 98/500\n",
      "8547/8547 [==============================] - 0s - loss: 0.5807 - acc: 0.7284     \n",
      "Epoch 99/500\n",
      "4000/8547 [=============>................] - ETA: 0s - loss: 0.5764 - acc: 0.7362"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-5ee970861d5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m history = model.fit(X_train500, Y_train,\n\u001b[1;32m     31\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                     verbose=1)\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/TONY/anaconda/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    406\u001b[0m                               \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m                               sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/Volumes/TONY/anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight)\u001b[0m\n\u001b[1;32m   1051\u001b[0m                               \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1053\u001b[0;31m                               callback_metrics=callback_metrics)\n\u001b[0m\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/TONY/anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics)\u001b[0m\n\u001b[1;32m    789\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    792\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/TONY/anaconda/lib/python2.7/site-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/TONY/anaconda/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/TONY/anaconda/lib/python2.7/site-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n)\u001b[0m\n\u001b[1;32m    912\u001b[0m             \u001b[0;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    915\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m                     \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/TONY/anaconda/lib/python2.7/site-packages/theano/tensor/blas.pyc\u001b[0m in \u001b[0;36mperform\u001b[0;34m(self, node, inp, out)\u001b[0m\n\u001b[1;32m   1550\u001b[0m         \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1552\u001b[0;31m             \u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1553\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m             \u001b[0;31m# The error raised by numpy has no shape information, we mean to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.utils import np_utils\n",
    "\n",
    "batch_size = 2000\n",
    "nb_classes = 2\n",
    "nb_epoch = 500\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(300, input_shape=(500,)))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train500, Y_train,\n",
    "                    batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "                    verbose=1, validation_data=(X_test500, Y_test))\n",
    "score = model.evaluate(X_test500, Y_test, verbose=0)\n",
    "\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "temp = abs(dfsMLP.selected_ws[0])\n",
    "\n",
    "plt.plot(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp = dfsMLP.selected_ws\n",
    "\n",
    "temp1, temp2 = temp[0], temp[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.where(abs(temp1)<0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.where(abs(temp2)<0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp = dfsMLP.selected_ws\n",
    "\n",
    "temp3, temp4 = temp[0], temp[1]\n",
    "\n",
    "print(np.where(abs(temp3)<0.01))\n",
    "print(np.where(abs(temp4)<0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp = dfsMLP.selected_ws\n",
    "\n",
    "temp5, temp6 = temp[0], temp[1]\n",
    "\n",
    "print(np.where(abs(temp5)<0.01))\n",
    "print(np.where(abs(temp6)<0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# rf = RandomForestClassifier(criterion=\"entropy\", n_estimators = 300, max_depth = 100)\n",
    "# rf.fit(X_train, y_train)\n",
    "\n",
    "# y_pred = rf.predict(X_test)\n",
    "\n",
    "gbm = xgb.XGBClassifier(max_depth=3, n_estimators=400, learning_rate=0.05).fit(X_train, y_train)\n",
    "y_pred = gbm.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "# gbm10 = xgb.XGBClassifier(max_depth=3, n_estimators=400, learning_rate=0.05).fit(X_train10, y_train)\n",
    "# y_pred10 = gbm.predict(X_test10)\n",
    "\n",
    "# print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indexes = np.argsort(gbm.feature_importances_)[::-1]\n",
    "\n",
    "# top100Features = np.array([columnNames[0][i] for i in indexes[0:200]])\n",
    "# top100Features = np.array([columnNames[i] for i in indexes[0:100]])\n",
    "# top100Features = top100Features.reshape(100,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train5, X_test5 = X_train[:, indexes[:5]], X_test[:, indexes[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "clf = SGDClassifier(loss=\"log\", penalty=\"l1\")\n",
    "\n",
    "clf.fit(X_train500, y_train)\n",
    "\n",
    "y_pred500lr = clf.predict(X_test500)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred500lr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
